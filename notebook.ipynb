{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Code\\Alzheimer-Disease-Diagnosis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Basic libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import plotly.express as px\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "import kagglehub\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
        "                           f1_score, confusion_matrix, classification_report,\n",
        "                           roc_curve, auc, roc_auc_score)\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "print(\"Basic libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: C:\\Users\\sajib\\.cache\\kagglehub\\datasets\\ninadaithal\\imagesoasis\\versions\\1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ninadaithal/imagesoasis\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using dataset path: C:\\Users\\sajib\\.cache\\kagglehub\\datasets\\ninadaithal\\imagesoasis\\versions\\1\\Data\n",
            "Classes found: ['Mild Dementia', 'Moderate Dementia', 'Non Demented', 'Very mild Dementia']\n",
            "Total images: 86437\n"
          ]
        }
      ],
      "source": [
        "# Resolve dataset path from KaggleHub `path`\n",
        "expected_classes = ['Mild Dementia', 'Very mild Dementia', 'Moderate Dementia', 'Non Demented']\n",
        "\n",
        "# Prefer .../versions/1/Data if present, else the root path\n",
        "candidates = [\n",
        "    os.path.join(path, 'Data'),\n",
        "    path\n",
        "]\n",
        "\n",
        "dataset_path = None\n",
        "found_classes = []\n",
        "for cand in candidates:\n",
        "    if os.path.isdir(cand):\n",
        "        subdirs = [d for d in os.listdir(cand) if os.path.isdir(os.path.join(cand, d))]\n",
        "        if any(cls in subdirs for cls in expected_classes):\n",
        "            dataset_path = cand\n",
        "            found_classes = [d for d in subdirs if d in expected_classes]\n",
        "            break\n",
        "\n",
        "if dataset_path:\n",
        "    print(f\"Using dataset path: {dataset_path}\")\n",
        "    print(f\"Classes found: {found_classes}\")\n",
        "    class_counts = {}\n",
        "    total_images = 0\n",
        "    for class_name in found_classes:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        image_files = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        class_counts[class_name] = len(image_files)\n",
        "        total_images += len(image_files)\n",
        "    print(f\"Total images: {total_images}\")\n",
        "else:\n",
        "    print(\"Could not resolve dataset path automatically. Please verify the download and directory structure.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset not found! Please ensure the OASIS dataset is downloaded.\n"
          ]
        }
      ],
      "source": [
        "def find_dataset_path():\n",
        "    \"\"\"Automatically find the OASIS dataset path\"\"\"\n",
        "    possible_paths = [\n",
        "        'C:/Users/sajib/OneDrive/Desktop/Alzheimer-Disease-Diagnosis/Data',\n",
        "        '/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1/Data',\n",
        "        '/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1',\n",
        "        'Data',\n",
        "        './Data',\n",
        "        '../Data'\n",
        "    ]\n",
        "    \n",
        "    expected_classes = ['Mild Dementia', 'Very mild Dementia', 'Moderate Dementia', 'Non Demented']\n",
        "    \n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
        "            if any(cls in subdirs for cls in expected_classes):\n",
        "                return path, subdirs\n",
        "    \n",
        "    return None, []\n",
        "dataset_path, found_classes = find_dataset_path()\n",
        "\n",
        "if dataset_path:\n",
        "    print(f\" Dataset found at: {dataset_path}\")\n",
        "    print(f\" Classes found: {found_classes}\")\n",
        "    \n",
        "    total_images = 0\n",
        "    class_counts = {}\n",
        "    \n",
        "    for class_name in found_classes:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            image_files = [f for f in os.listdir(class_dir) \n",
        "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "            count = len(image_files)\n",
        "            class_counts[class_name] = count\n",
        "            total_images += count\n",
        "            print(f\"   {class_name}: {count:,} images\")\n",
        "    \n",
        "    print(f\"Total images: {total_images:,}\")\n",
        "else:\n",
        "    print(\"Dataset not found! Please ensure the OASIS dataset is downloaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset not found! Please ensure the OASIS dataset is downloaded.\n"
          ]
        }
      ],
      "source": [
        "def find_dataset_path():\n",
        "    \"\"\"Automatically find the OASIS dataset path\"\"\"\n",
        "    possible_paths = [\n",
        "        'C:/Users/sajib/OneDrive/Desktop/Alzheimer-Disease-Diagnosis/Data',\n",
        "        '/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1/Data',\n",
        "        '/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1',\n",
        "        'Data',\n",
        "        './Data',\n",
        "        '../Data'\n",
        "    ]\n",
        "    \n",
        "    expected_classes = ['Mild Dementia', 'Very mild Dementia', 'Moderate Dementia', 'Non Demented']\n",
        "    \n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
        "            if any(cls in subdirs for cls in expected_classes):\n",
        "                return path, subdirs\n",
        "    \n",
        "    return None, []\n",
        "dataset_path, found_classes = find_dataset_path()\n",
        "\n",
        "if dataset_path:\n",
        "    print(f\" Dataset found at: {dataset_path}\")\n",
        "    print(f\" Classes found: {found_classes}\")\n",
        "    \n",
        "    total_images = 0\n",
        "    class_counts = {}\n",
        "    \n",
        "    for class_name in found_classes:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            image_files = [f for f in os.listdir(class_dir) \n",
        "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "            count = len(image_files)\n",
        "            class_counts[class_name] = count\n",
        "            total_images += count\n",
        "            print(f\"   {class_name}: {count:,} images\")\n",
        "    \n",
        "    print(f\"Total images: {total_images:,}\")\n",
        "else:\n",
        "    print(\"Dataset not found! Please ensure the OASIS dataset is downloaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset not found! Please ensure the OASIS dataset is downloaded.\n"
          ]
        }
      ],
      "source": [
        "def find_dataset_path():\n",
        "    \"\"\"Automatically find the OASIS dataset path\"\"\"\n",
        "    possible_paths = [\n",
        "        'C:/Users/sajib/OneDrive/Desktop/Alzheimer-Disease-Diagnosis/Data',\n",
        "        '/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1/Data',\n",
        "        '/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1',\n",
        "        'Data',\n",
        "        './Data',\n",
        "        '../Data'\n",
        "    ]\n",
        "    \n",
        "    expected_classes = ['Mild Dementia', 'Very mild Dementia', 'Moderate Dementia', 'Non Demented']\n",
        "    \n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
        "            if any(cls in subdirs for cls in expected_classes):\n",
        "                return path, subdirs\n",
        "    \n",
        "    return None, []\n",
        "dataset_path, found_classes = find_dataset_path()\n",
        "\n",
        "if dataset_path:\n",
        "    print(f\" Dataset found at: {dataset_path}\")\n",
        "    print(f\" Classes found: {found_classes}\")\n",
        "    \n",
        "    total_images = 0\n",
        "    class_counts = {}\n",
        "    \n",
        "    for class_name in found_classes:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            image_files = [f for f in os.listdir(class_dir) \n",
        "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "            count = len(image_files)\n",
        "            class_counts[class_name] = count\n",
        "            total_images += count\n",
        "            print(f\"   {class_name}: {count:,} images\")\n",
        "    \n",
        "    print(f\"Total images: {total_images:,}\")\n",
        "else:\n",
        "    print(\"Dataset not found! Please ensure the OASIS dataset is downloaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset not found! Please ensure the OASIS dataset is downloaded.\n"
          ]
        }
      ],
      "source": [
        "def find_dataset_path():\n",
        "    \"\"\"Automatically find the OASIS dataset path\"\"\"\n",
        "    possible_paths = [\n",
        "        'C:/Users/sajib/OneDrive/Desktop/Alzheimer-Disease-Diagnosis/Data',\n",
        "        '/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1/Data',\n",
        "        '/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1',\n",
        "        'Data',\n",
        "        './Data',\n",
        "        '../Data'\n",
        "    ]\n",
        "    \n",
        "    expected_classes = ['Mild Dementia', 'Very mild Dementia', 'Moderate Dementia', 'Non Demented']\n",
        "    \n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
        "            if any(cls in subdirs for cls in expected_classes):\n",
        "                return path, subdirs\n",
        "    \n",
        "    return None, []\n",
        "dataset_path, found_classes = find_dataset_path()\n",
        "\n",
        "if dataset_path:\n",
        "    print(f\" Dataset found at: {dataset_path}\")\n",
        "    print(f\" Classes found: {found_classes}\")\n",
        "    \n",
        "    total_images = 0\n",
        "    class_counts = {}\n",
        "    \n",
        "    for class_name in found_classes:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            image_files = [f for f in os.listdir(class_dir) \n",
        "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "            count = len(image_files)\n",
        "            class_counts[class_name] = count\n",
        "            total_images += count\n",
        "            print(f\"   {class_name}: {count:,} images\")\n",
        "    \n",
        "    print(f\"Total images: {total_images:,}\")\n",
        "else:\n",
        "    print(\"Dataset not found! Please ensure the OASIS dataset is downloaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset not found! Please ensure the OASIS dataset is downloaded.\n"
          ]
        }
      ],
      "source": [
        "def find_dataset_path():\n",
        "    \"\"\"Automatically find the OASIS dataset path\"\"\"\n",
        "    possible_paths = [\n",
        "        'C:/Users/sajib/OneDrive/Desktop/Alzheimer-Disease-Diagnosis/Data',\n",
        "        '/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1/Data',\n",
        "        '/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1',\n",
        "        'Data',\n",
        "        './Data',\n",
        "        '../Data'\n",
        "    ]\n",
        "    \n",
        "    expected_classes = ['Mild Dementia', 'Very mild Dementia', 'Moderate Dementia', 'Non Demented']\n",
        "    \n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
        "            if any(cls in subdirs for cls in expected_classes):\n",
        "                return path, subdirs\n",
        "    \n",
        "    return None, []\n",
        "dataset_path, found_classes = find_dataset_path()\n",
        "\n",
        "if dataset_path:\n",
        "    print(f\" Dataset found at: {dataset_path}\")\n",
        "    print(f\" Classes found: {found_classes}\")\n",
        "    \n",
        "    total_images = 0\n",
        "    class_counts = {}\n",
        "    \n",
        "    for class_name in found_classes:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            image_files = [f for f in os.listdir(class_dir) \n",
        "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "            count = len(image_files)\n",
        "            class_counts[class_name] = count\n",
        "            total_images += count\n",
        "            print(f\"   {class_name}: {count:,} images\")\n",
        "    \n",
        "    print(f\"Total images: {total_images:,}\")\n",
        "else:\n",
        "    print(\"Dataset not found! Please ensure the OASIS dataset is downloaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cannot visualize - dataset not found\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if dataset_path and class_counts:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    classes = list(class_counts.keys())\n",
        "    counts = list(class_counts.values())\n",
        "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
        "    \n",
        "    bars = ax1.bar(classes, counts, color=colors)\n",
        "    ax1.set_title('Alzheimer MRI Dataset - Class Distribution', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Dementia Severity')\n",
        "    ax1.set_ylabel('Number of Images')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    for bar, count in zip(bars, counts):\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + max(counts)*0.01,\n",
        "                f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    ax2.pie(counts, labels=classes, autopct='%1.1f%%', startangle=90, colors=colors)\n",
        "    ax2.set_title('Class Proportions', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f\"Dataset Statistics:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Total images: {sum(counts):,}\")\n",
        "    print(f\"Number of classes: {len(classes)}\")\n",
        "    print(f\"Average images per class: {np.mean(counts):,.0f}\")\n",
        "    print(f\"Std deviation: {np.std(counts):,.0f}\")\n",
        "    print(f\"Min images in a class: {min(counts):,}\")\n",
        "    print(f\"Max images in a class: {max(counts):,}\")\n",
        "    \n",
        "    imbalance_ratio = max(counts) / min(counts)\n",
        "    print(f\"Class imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "    \n",
        "    if imbalance_ratio > 5:\n",
        "        print(\" Significant class imbalance detected - will use weighted loss for training\")\n",
        "    else:\n",
        "        print(\"Moderate class imbalance - manageable with weighted loss for training\")\n",
        "else:\n",
        "    print(\"Cannot visualize - dataset not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "possible_paths = [\n",
        "    '/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1/Data',\n",
        "    '/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1',\n",
        "    'Data',\n",
        "    './Data',\n",
        "    '../Data'\n",
        "]\n",
        "\n",
        "dataset_path = None\n",
        "for path in possible_paths:\n",
        "    if os.path.exists(path):\n",
        "        subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
        "        expected_classes = ['Mild Dementia', 'Very mild Dementia', 'Moderate Dementia', 'Non Demented']\n",
        "        \n",
        "        if any(cls in subdirs for cls in expected_classes):\n",
        "            dataset_path = path\n",
        "            print(f\"Dataset found at: {dataset_path}\")\n",
        "            print(f\"Subdirectories: {subdirs}\")\n",
        "            break\n",
        "\n",
        "if dataset_path is None:\n",
        "    print(\"Dataset not found automatically. Please update the dataset_path variable manually.\")\n",
        "    print(\"Expected structure:\")\n",
        "    print(\"Data/\")\n",
        "    print(\"  Mild Dementia/\")\n",
        "    print(\"  Very mild Dementia/\")\n",
        "    print(\"  Moderate Dementia/\")\n",
        "    print(\"  Non Demented/\")\n",
        "else:\n",
        "    print(f\"Using dataset path: {dataset_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def explore_dataset(dataset_path):\n",
        "    \"\"\"Explore the dataset and return information about classes and image counts\"\"\"\n",
        "    \n",
        "    if dataset_path is None:\n",
        "        print(\"Please set the correct dataset_path in the previous cell\")\n",
        "        return None\n",
        "    \n",
        "    class_names = [d for d in os.listdir(dataset_path) \n",
        "                   if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "    \n",
        "    print(f\"Found {len(class_names)} classes:\")\n",
        "    \n",
        "    class_info = {}\n",
        "    total_images = 0\n",
        "    \n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        image_files = [f for f in os.listdir(class_dir) \n",
        "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        \n",
        "        class_info[class_name] = {\n",
        "            'count': len(image_files),\n",
        "            'sample_files': image_files[:3] \n",
        "        }\n",
        "        total_images += len(image_files)\n",
        "        \n",
        "        print(f\"  {class_name}: {len(image_files)} images\")\n",
        "        print(f\"    Sample files: {image_files[:3]}\")\n",
        "    \n",
        "    print(f\"\\nTotal images: {total_images}\")\n",
        "    \n",
        "    return {\n",
        "        'class_names': class_names,\n",
        "        'class_info': class_info,\n",
        "        'total_images': total_images,\n",
        "        'dataset_path': dataset_path\n",
        "    }\n",
        "\n",
        "\n",
        "dataset_info = explore_dataset(dataset_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if dataset_info is not None:\n",
        "    class_names = dataset_info['class_names']\n",
        "    class_counts = [dataset_info['class_info'][name]['count'] for name in class_names]\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    bars = ax1.bar(class_names, class_counts, color=['skyblue', 'lightgreen', 'orange', 'salmon'])\n",
        "    ax1.set_title('Number of Images per Class', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Class')\n",
        "    ax1.set_ylabel('Number of Images')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    for bar, count in zip(bars, class_counts):\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 50,\n",
        "                f'{count}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    \n",
        "    ax2.pie(class_counts, labels=class_names, autopct='%1.1f%%', startangle=140,\n",
        "            colors=['skyblue', 'lightgreen', 'orange', 'salmon'])\n",
        "    ax2.set_title('Class Proportions', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Dataset Statistics:\")\n",
        "    print(f\"Total images: {sum(class_counts):,}\")\n",
        "    print(f\"Number of classes: {len(class_names)}\")\n",
        "    print(f\"Average images per class: {np.mean(class_counts):.1f}\")\n",
        "    print(f\"Min images in a class: {min(class_counts)}\")\n",
        "    print(f\"Max images in a class: {max(class_counts)}\")\n",
        "    \n",
        "    imbalance_ratio = max(class_counts) / min(class_counts)\n",
        "    print(f\"Class imbalance ratio: {imbalance_ratio:.2f}\")\n",
        "else:\n",
        "    print(\"Please fix the dataset path issue first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def display_sample_images(dataset_info, samples_per_class=3):\n",
        "    if dataset_info is None:\n",
        "        print(\"Dataset info not available\")\n",
        "        return\n",
        "    \n",
        "    class_names = dataset_info['class_names']\n",
        "    dataset_path = dataset_info['dataset_path']\n",
        "    \n",
        "    fig, axes = plt.subplots(len(class_names), samples_per_class, \n",
        "                            figsize=(samples_per_class * 4, len(class_names) * 3))\n",
        "    \n",
        "    if len(class_names) == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    \n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        image_files = [f for f in os.listdir(class_dir) \n",
        "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        \n",
        "        sample_files = random.sample(image_files, min(samples_per_class, len(image_files)))\n",
        "        \n",
        "        for j, img_file in enumerate(sample_files):\n",
        "            img_path = os.path.join(class_dir, img_file)\n",
        "            try:\n",
        "                img = Image.open(img_path)\n",
        "                axes[i, j].imshow(img, cmap='gray')\n",
        "                axes[i, j].set_title(f\"{class_name}\\\\n{img_file}\", fontsize=10)\n",
        "                axes[i, j].axis('off')\n",
        "            except Exception as e:\n",
        "                axes[i, j].text(0.5, 0.5, f\"Error loading\\\\n{img_file}\", \n",
        "                               ha='center', va='center', transform=axes[i, j].transAxes)\n",
        "                axes[i, j].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if dataset_info is not None:\n",
        "    print(\"Sample images from each class:\")\n",
        "    display_sample_images(dataset_info, samples_per_class=4)\n",
        "else:\n",
        "    print(\"Please fix the dataset path first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_image_properties(dataset_info, sample_size=100):\n",
        "    \"\"\"Analyze image properties like size, format, etc.\"\"\"\n",
        "    \n",
        "    if dataset_info is None:\n",
        "        print(\"Dataset info not available\")\n",
        "        return None\n",
        "    \n",
        "    class_names = dataset_info['class_names']\n",
        "    dataset_path = dataset_info['dataset_path']\n",
        "    \n",
        "    image_properties = []\n",
        "    \n",
        "    print(\"Analyzing image properties...\")\n",
        "    \n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        image_files = [f for f in os.listdir(class_dir) \n",
        "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        \n",
        "        sample_files = random.sample(image_files, min(sample_size, len(image_files)))\n",
        "        \n",
        "        for img_file in sample_files:\n",
        "            img_path = os.path.join(class_dir, img_file)\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    properties = {\n",
        "                        'class': class_name,\n",
        "                        'filename': img_file,\n",
        "                        'width': img.width,\n",
        "                        'height': img.height,\n",
        "                        'mode': img.mode,\n",
        "                        'format': img.format\n",
        "                    }\n",
        "                    image_properties.append(properties)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {img_path}: {e}\")\n",
        "    \n",
        "    df = pd.DataFrame(image_properties)\n",
        "    \n",
        "    print(f\"\\\\nAnalyzed {len(df)} images\")\n",
        "    print(f\"Image formats found: {df['format'].unique()}\")\n",
        "    print(f\"Image modes found: {df['mode'].unique()}\")\n",
        "    \n",
        "    print(f\"\\\\nImage size statistics:\")\n",
        "    print(f\"Width - Min: {df['width'].min()}, Max: {df['width'].max()}, Mean: {df['width'].mean():.1f}\")\n",
        "    print(f\"Height - Min: {df['height'].min()}, Max: {df['height'].max()}, Mean: {df['height'].mean():.1f}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "if dataset_info is not None:\n",
        "    image_props_df = analyze_image_properties(dataset_info, sample_size=50)\n",
        "else:\n",
        "    print(\"Please fix the dataset path first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if 'image_props_df' in locals() and image_props_df is not None:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    for class_name in image_props_df['class'].unique():\n",
        "        class_data = image_props_df[image_props_df['class'] == class_name]\n",
        "        axes[0, 0].hist(class_data['width'], alpha=0.7, label=class_name, bins=20)\n",
        "    axes[0, 0].set_title('Image Width Distribution by Class')\n",
        "    axes[0, 0].set_xlabel('Width (pixels)')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "    axes[0, 0].legend()\n",
        "    \n",
        "    for class_name in image_props_df['class'].unique():\n",
        "        class_data = image_props_df[image_props_df['class'] == class_name]\n",
        "        axes[0, 1].hist(class_data['height'], alpha=0.7, label=class_name, bins=20)\n",
        "    axes[0, 1].set_title('Image Height Distribution by Class')\n",
        "    axes[0, 1].set_xlabel('Height (pixels)')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    axes[0, 1].legend()\n",
        "    \n",
        "    sns.boxplot(data=image_props_df, x='class', y='width', ax=axes[1, 0])\n",
        "    axes[1, 0].set_title('Width Distribution by Class (Boxplot)')\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    sns.boxplot(data=image_props_df, x='class', y='height', ax=axes[1, 1])\n",
        "    axes[1, 1].set_title('Height Distribution by Class (Boxplot)')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    unique_sizes = image_props_df[['width', 'height']].drop_duplicates()\n",
        "    print(f\"Number of unique image sizes: {len(unique_sizes)}\")\n",
        "    if len(unique_sizes) <= 5:\n",
        "        print(\"Most common image sizes:\")\n",
        "        size_counts = image_props_df.groupby(['width', 'height']).size().sort_values(ascending=False)\n",
        "        print(size_counts.head())\n",
        "    else:\n",
        "        print(\"Images have varying sizes - resizing will be needed for model training\")\n",
        "else:\n",
        "    print(\"Image properties data not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: C:\\Users\\sajib\\.cache\\kagglehub\\datasets\\ninadaithal\\imagesoasis\\versions\\1\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "path = kagglehub.dataset_download(\"ninadaithal/imagesoasis\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "dataset_path = \"/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1/Data\"\n",
        "\n",
        "classes = ['Non Demented', 'Very mild Dementia', 'Mild Dementia', 'Moderate Dementia']\n",
        "class_counts = {cls: len(glob.glob(f\"{dataset_path}/{cls}/*.jpg\")) for cls in classes}\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))\n",
        "plt.title('Alzheimer MRI Class Distribution')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "dimension_counts = {cls: {} for cls in classes}\n",
        "\n",
        "for cls in classes:\n",
        "    cls_path = os.path.join(dataset_path, cls)\n",
        "    image_files = glob.glob(os.path.join(cls_path, '*.jpg'))\n",
        "    \n",
        "    for img_file in image_files:\n",
        "        with Image.open(img_file) as img:\n",
        "            dimensions = img.size  \n",
        "            if dimensions in dimension_counts[cls]:\n",
        "                dimension_counts[cls][dimensions] += 1\n",
        "            else:\n",
        "                dimension_counts[cls][dimensions] = 1\n",
        "\n",
        "\n",
        "x_labels = list(classes)\n",
        "x_pos = np.arange(len(x_labels))\n",
        "y_labels = set()\n",
        "for cls in classes:\n",
        "    y_labels.update(dimension_counts[cls].keys())\n",
        "y_labels = sorted(y_labels)\n",
        "y_pos = np.arange(len(y_labels))\n",
        "\n",
        "x, y = np.meshgrid(x_pos, y_pos)\n",
        "z = np.zeros_like(x)\n",
        "\n",
        "\n",
        "for i, cls in enumerate(classes):\n",
        "    for j, dim in enumerate(y_labels):\n",
        "        z[j, i] = dimension_counts[cls].get(dim, 0)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "\n",
        "dx = dy = 0.5\n",
        "dz = z.flatten()\n",
        "ax.bar3d(x.flatten(), y.flatten(), np.zeros_like(dz), dx, dy, dz, shade=True)\n",
        "\n",
        "\n",
        "ax.set_xlabel('Class')\n",
        "ax.set_ylabel('Image Dimensions (Width x Height)')\n",
        "ax.set_zlabel('Count')\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels([f'{dim[0]}x{dim[1]}' for dim in y_labels], rotation=45, ha='right')\n",
        "\n",
        "plt.title('3D Bar Chart: Count vs Class vs Image Dimensions')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_data = {\n",
        "    'Class': list(class_counts.keys()),\n",
        "    'Count': list(class_counts.values())\n",
        "}\n",
        "\n",
        "df_class = pd.DataFrame(class_data)\n",
        "\n",
        "fig = px.treemap(\n",
        "    df_class,\n",
        "    path=['Class'],\n",
        "    values='Count',\n",
        "    title='Treemap: Class Proportions',\n",
        "    color='Class',\n",
        "    color_discrete_sequence=px.colors.qualitative.Pastel\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes found: ['Data']\n",
            "Data: 0 images\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m image_files = [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os.listdir(class_dir) \u001b[38;5;28;01mif\u001b[39;00m f.endswith(\u001b[33m'\u001b[39m\u001b[33m.jpg\u001b[39m\u001b[33m'\u001b[39m)]\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     img_path = os.path.join(class_dir, \u001b[43mimage_files\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     18\u001b[39m     img = Image.open(img_path)\n\u001b[32m     19\u001b[39m     axes[i, j].imshow(img, cmap=\u001b[33m'\u001b[39m\u001b[33mgray\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mIndexError\u001b[39m: list index out of range"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x300 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset_path = \"C:/Users/sajib/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1\"\n",
        "\n",
        "class_names = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "print(\"Classes found:\", class_names)\n",
        "\n",
        "\n",
        "for class_name in class_names:\n",
        "    class_dir = os.path.join(dataset_path, class_name)\n",
        "    num_images = len([f for f in os.listdir(class_dir) if f.endswith('.jpg')])\n",
        "    print(f\"{class_name}: {num_images} images\")\n",
        "\n",
        "fig, axes = plt.subplots(len(class_names), 3, figsize=(12, 3 * len(class_names)))\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_dir = os.path.join(dataset_path, class_name)\n",
        "    image_files = [f for f in os.listdir(class_dir) if f.endswith('.jpg')]\n",
        "    for j in range(3):\n",
        "        img_path = os.path.join(class_dir, image_files[j])\n",
        "        img = Image.open(img_path)\n",
        "        axes[i, j].imshow(img, cmap='gray')\n",
        "        axes[i, j].set_title(f\"{class_name}\\n{image_files[j]}\")\n",
        "        axes[i, j].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "image_shapes = []\n",
        "for class_name in class_names:\n",
        "    class_dir = os.path.join(dataset_path, class_name)\n",
        "    for fname in os.listdir(class_dir):\n",
        "        if fname.endswith('.jpg'):\n",
        "            img = Image.open(os.path.join(class_dir, fname))\n",
        "            image_shapes.append(img.size)\n",
        "\n",
        "\n",
        "widths, heights = zip(*image_shapes)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(widths, bins=20, color='orange')\n",
        "plt.title('Image Width Distribution')\n",
        "plt.xlabel('Width (pixels)')\n",
        "plt.ylabel('Count')\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist(heights, bins=20, color='green')\n",
        "plt.title('Image Height Distribution')\n",
        "plt.xlabel('Height (pixels)')\n",
        "plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "for class_name in class_names:\n",
        "    class_dir = os.path.join(dataset_path, class_name)\n",
        "    pixels = []\n",
        "    for fname in os.listdir(class_dir)[:10]:  \n",
        "        img = Image.open(os.path.join(class_dir, fname)).convert('L')\n",
        "        pixels.extend(np.array(img).flatten())\n",
        "    plt.hist(pixels, bins=50, alpha=0.5, label=class_name)\n",
        "plt.title('Pixel Intensity Distribution by Class')\n",
        "plt.xlabel('Pixel Intensity')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Dataset class and transforms defined successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class AlzheimerDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        \n",
        "        self.class_to_idx = {\n",
        "            'Non Demented': 0,\n",
        "            'Very mild Dementia': 1,\n",
        "            'Mild Dementia': 2,\n",
        "            'Moderate Dementia': 3\n",
        "        }\n",
        "        \n",
        "        if len(self.labels) > 0 and isinstance(self.labels[0], str):\n",
        "            self.labels = [self.class_to_idx[label] for label in self.labels]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path).convert('L')  # keep grayscale; transforms will expand to 3ch\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n",
        "\n",
        "def get_transforms(img_size=224, is_training=False):\n",
        "    \"\"\"Medical-safe transforms; avoid color/hue changes; keep geometry modest; ensure 3-channel grayscale\"\"\"\n",
        "    if is_training:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(degrees=10),\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "            transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "print(\" Dataset class and transforms defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using dataset path: C:\\Users\\sajib\\.cache\\kagglehub\\datasets\\ninadaithal\\imagesoasis\\versions\\1\\Data\n",
            "Classes: ['Mild Dementia', 'Moderate Dementia', 'Non Demented', 'Very mild Dementia']\n",
            " Loading and splitting dataset...\n",
            " Loading data from 4 classes...\n",
            " Total images loaded: 86,437\n",
            "   Mild Dementia: 5,002 images (5.8%)\n",
            "   Moderate Dementia: 488 images (0.6%)\n",
            "   Non Demented: 67,222 images (77.8%)\n",
            "   Very mild Dementia: 13,725 images (15.9%)\n",
            "\\n Dataset split completed:\n",
            "   Train: 60,505 images (70.0%)\n",
            "   Validation: 12,966 images (15.0%)\n",
            "   Test: 12,966 images (15.0%)\n",
            " Creating data loaders...\n",
            " Class weights: [ 0.32146576  1.574503    4.3193173  44.2288    ]\n",
            " Class mapping (class_to_idx): {'Non Demented': 0, 'Very mild Dementia': 1, 'Mild Dementia': 2, 'Moderate Dementia': 3}\n",
            " Train class counts: [47054.0, 9607.0, 3502.0, 342.0]\n",
            " Data loaders created successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def load_and_split_data(dataset_path, class_names, test_size=0.15, val_size=0.15, random_state=42):\n",
        "    \"\"\"Load dataset and create train/val/test splits (70-15-15)\"\"\"\n",
        "    \n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    \n",
        "    print(f\" Loading data from {len(class_names)} classes...\")\n",
        "    \n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        if not os.path.exists(class_dir):\n",
        "            continue\n",
        "            \n",
        "        image_files = [f for f in os.listdir(class_dir) \n",
        "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        \n",
        "        for img_file in image_files:\n",
        "            image_paths.append(os.path.join(class_dir, img_file))\n",
        "            labels.append(class_name)\n",
        "    \n",
        "    print(f\" Total images loaded: {len(image_paths):,}\")\n",
        "    \n",
        "    class_dist = Counter(labels)\n",
        "    for class_name, count in class_dist.items():\n",
        "        print(f\"   {class_name}: {count:,} images ({count/len(labels)*100:.1f}%)\")\n",
        "    \n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        image_paths, labels, \n",
        "        test_size=test_size, \n",
        "        stratify=labels, \n",
        "        random_state=random_state\n",
        "    )\n",
        "    \n",
        "    val_size_adjusted = val_size / (1 - test_size)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, \n",
        "        test_size=val_size_adjusted, \n",
        "        stratify=y_temp, \n",
        "        random_state=random_state\n",
        "    )\n",
        "    \n",
        "    print(f\"\\\\n Dataset split completed:\")\n",
        "    print(f\"   Train: {len(X_train):,} images ({len(X_train)/len(image_paths)*100:.1f}%)\")\n",
        "    print(f\"   Validation: {len(X_val):,} images ({len(X_val)/len(image_paths)*100:.1f}%)\")\n",
        "    print(f\"   Test: {len(X_test):,} images ({len(X_test)/len(image_paths)*100:.1f}%)\")\n",
        "    \n",
        "    return {\n",
        "        'train': {'paths': X_train, 'labels': y_train},\n",
        "        'val': {'paths': X_val, 'labels': y_val},\n",
        "        'test': {'paths': X_test, 'labels': y_test},\n",
        "        'class_names': class_names,\n",
        "        'class_counts': class_dist\n",
        "    }\n",
        "\n",
        "def create_data_loaders(data_splits, batch_size=32, img_size=224, num_workers=0):\n",
        "    \"\"\"Create data loaders (num_workers=0 to avoid multiprocessing issues)\"\"\"\n",
        "\n",
        "    train_transform = get_transforms(img_size=img_size, is_training=True)\n",
        "    val_transform = get_transforms(img_size=img_size, is_training=False)\n",
        "\n",
        "    train_dataset = AlzheimerDataset(\n",
        "        data_splits['train']['paths'],\n",
        "        data_splits['train']['labels'],\n",
        "        transform=train_transform\n",
        "    )\n",
        "\n",
        "    val_dataset = AlzheimerDataset(\n",
        "        data_splits['val']['paths'],\n",
        "        data_splits['val']['labels'],\n",
        "        transform=val_transform\n",
        "    )\n",
        "\n",
        "    test_dataset = AlzheimerDataset(\n",
        "        data_splits['test']['paths'],\n",
        "        data_splits['test']['labels'],\n",
        "        transform=val_transform\n",
        "    )\n",
        "\n",
        "    # Robust class weights aligned with number of classes\n",
        "    num_classes = 4\n",
        "    train_label_tensor = torch.tensor(train_dataset.labels, dtype=torch.long)\n",
        "    class_counts = torch.bincount(train_label_tensor, minlength=num_classes).float()\n",
        "    class_counts[class_counts == 0] = 1.0\n",
        "    weights = (class_counts.sum() / (num_classes * class_counts)).numpy()\n",
        "    class_weights = torch.tensor(weights, dtype=torch.float)\n",
        "\n",
        "    print(f\" Class weights: {class_weights.numpy()}\")\n",
        "    print(f\" Class mapping (class_to_idx): {train_dataset.class_to_idx}\")\n",
        "    print(f\" Train class counts: {class_counts.tolist()}\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True, \n",
        "        num_workers=num_workers, pin_memory=False\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=False, \n",
        "        num_workers=num_workers, pin_memory=False\n",
        "    )\n",
        "    \n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=False, \n",
        "        num_workers=num_workers, pin_memory=False\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'train': train_loader,\n",
        "        'val': val_loader,\n",
        "        'test': test_loader,\n",
        "        'class_weights': class_weights\n",
        "    }\n",
        "\n",
        "# Re-resolve dataset path and classes if needed (avoids earlier cells overriding)\n",
        "expected = ['Mild Dementia', 'Very mild Dementia', 'Moderate Dementia', 'Non Demented']\n",
        "\n",
        "def resolve_dataset_from_path(path_var):\n",
        "    candidates = [os.path.join(path_var, 'Data'), path_var]\n",
        "    for cand in candidates:\n",
        "        if os.path.isdir(cand):\n",
        "            subdirs = [d for d in os.listdir(cand) if os.path.isdir(os.path.join(cand, d))]\n",
        "            found = [d for d in subdirs if d in expected]\n",
        "            if len(found) > 0:\n",
        "                return cand, found\n",
        "    return None, []\n",
        "\n",
        "if (not dataset_path) or (not os.path.isdir(dataset_path)):\n",
        "    dataset_path, found_classes = resolve_dataset_from_path(path)\n",
        "else:\n",
        "    # If classes are missing, attempt to infer again\n",
        "    subdirs = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "    if not any(c in subdirs for c in expected):\n",
        "        dataset_path, found_classes = resolve_dataset_from_path(path)\n",
        "    else:\n",
        "        if 'found_classes' not in locals() or not found_classes:\n",
        "            found_classes = [d for d in subdirs if d in expected]\n",
        "\n",
        "if dataset_path:\n",
        "    class_list = found_classes if (found_classes and len(found_classes) > 0) else [\n",
        "        d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d)) and d in expected\n",
        "    ]\n",
        "    if len(class_list) == 0:\n",
        "        print(f\"Dataset path detected but no expected class folders found under: {dataset_path}\")\n",
        "    else:\n",
        "        print(f\"Using dataset path: {dataset_path}\")\n",
        "        print(f\"Classes: {class_list}\")\n",
        "        print(\" Loading and splitting dataset...\")\n",
        "        data_splits = load_and_split_data(dataset_path, class_list)\n",
        "        print(\" Creating data loaders...\")\n",
        "        data_loaders = create_data_loaders(data_splits, batch_size=16, img_size=224)\n",
        "        print(\" Data loaders created successfully!\")\n",
        "else:\n",
        "    print(\"Cannot create data loaders - dataset not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Testing data loaders...\n",
            " Train: Batch shape torch.Size([16, 3, 224, 224]), Labels shape torch.Size([16])\n",
            "   Label range: 0-2\n",
            "   Image value range: -2.118 to 2.640\n",
            " Val: Batch shape torch.Size([16, 3, 224, 224]), Labels shape torch.Size([16])\n",
            "   Label range: 0-2\n",
            "   Image value range: -2.118 to 2.396\n",
            " Test: Batch shape torch.Size([16, 3, 224, 224]), Labels shape torch.Size([16])\n",
            "   Label range: 0-2\n",
            "   Image value range: -2.118 to 2.501\n",
            "\\n All data loaders working correctly!\n"
          ]
        }
      ],
      "source": [
        "if 'data_loaders' in locals():\n",
        "    print(\" Testing data loaders...\")\n",
        "    \n",
        "    for phase in ['train', 'val', 'test']:\n",
        "        loader = data_loaders[phase]\n",
        "        \n",
        "        try:\n",
        "            for images, labels in loader:\n",
        "                print(f\" {phase.capitalize()}: Batch shape {images.shape}, Labels shape {labels.shape}\")\n",
        "                print(f\"   Label range: {labels.min().item()}-{labels.max().item()}\")\n",
        "                print(f\"   Image value range: {images.min().item():.3f} to {images.max().item():.3f}\")\n",
        "                break\n",
        "        except Exception as e:\n",
        "            print(f\" Error in {phase} loader: {e}\")\n",
        "    \n",
        "    print(\"\\\\n All data loaders working correctly!\")\n",
        "else:\n",
        "    print(\" Data loaders not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Phase 1 models to train: ['vgg16', 'vgg19', 'resnet50', 'resnet101', 'resnet152', 'densenet121', 'densenet201', 'mobilenetv3_large', 'shufflenet_v2_x1_0']\n",
            "\\n Testing model creation...\n",
            " Creating vgg16 model...\n",
            " vgg16: 134.3M total params, 134.3M trainable\n",
            " Creating vgg19 model...\n",
            " vgg19: 139.6M total params, 139.6M trainable\n",
            " Creating resnet50 model...\n",
            " resnet50: 23.5M total params, 23.5M trainable\n",
            " Creating resnet101 model...\n",
            " resnet101: 42.5M total params, 42.5M trainable\n",
            " Creating resnet152 model...\n",
            " resnet152: 58.2M total params, 58.2M trainable\n",
            " Creating densenet121 model...\n",
            " densenet121: 7.0M total params, 7.0M trainable\n",
            " Creating densenet201 model...\n",
            " densenet201: 18.1M total params, 18.1M trainable\n",
            " Creating mobilenetv3_large model...\n",
            " mobilenetv3_large: 4.2M total params, 4.2M trainable\n"
          ]
        }
      ],
      "source": [
        "def create_model(model_name, num_classes=4, pretrained=True):\n",
        "    \n",
        "    print(f\" Creating {model_name} model...\")\n",
        "    \n",
        "    if model_name.startswith('vgg'):\n",
        "        if model_name == 'vgg16':\n",
        "            model = models.vgg16(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "            model.classifier[6] = nn.Linear(4096, num_classes)\n",
        "        elif model_name == 'vgg19':\n",
        "            model = models.vgg19(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "            model.classifier[6] = nn.Linear(4096, num_classes)\n",
        "            \n",
        "    elif model_name.startswith('resnet'):\n",
        "        if model_name == 'resnet50':\n",
        "            model = models.resnet50(weights='IMAGENET1K_V2' if pretrained else None)\n",
        "        elif model_name == 'resnet101':\n",
        "            model = models.resnet101(weights='IMAGENET1K_V2' if pretrained else None)\n",
        "        elif model_name == 'resnet152':\n",
        "            model = models.resnet152(weights='IMAGENET1K_V2' if pretrained else None)\n",
        "        \n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "        \n",
        "    elif model_name.startswith('densenet'):\n",
        "        if model_name == 'densenet121':\n",
        "            model = models.densenet121(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "        elif model_name == 'densenet201':\n",
        "            model = models.densenet201(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "        \n",
        "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
        "        \n",
        "    elif model_name == 'mobilenetv3_large':\n",
        "        model = models.mobilenet_v3_large(weights='IMAGENET1K_V2' if pretrained else None)\n",
        "        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
        "        \n",
        "    elif model_name == 'shufflenet_v2_x1_0':\n",
        "        model = models.shufflenet_v2_x1_0(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "        \n",
        "    else:\n",
        "        raise ValueError(f\"Model {model_name} not supported\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "PHASE1_MODELS = [\n",
        "    'vgg16',\n",
        "    'vgg19', \n",
        "    'resnet50',\n",
        "    'resnet101',\n",
        "    'resnet152',\n",
        "    'densenet121',\n",
        "    'densenet201',\n",
        "    'mobilenetv3_large',\n",
        "    'shufflenet_v2_x1_0'\n",
        "]\n",
        "\n",
        "print(f\" Phase 1 models to train: {PHASE1_MODELS}\")\n",
        "\n",
        "print(\"\\\\n Testing model creation...\")\n",
        "for model_name in PHASE1_MODELS[:8]: \n",
        "    try:\n",
        "        model = create_model(model_name, num_classes=4, pretrained=False)\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        print(f\" {model_name}: {total_params/1e6:.1f}M total params, {trainable_params/1e6:.1f}M trainable\")\n",
        "        del model\n",
        "    except Exception as e:\n",
        "        print(f\" Error creating {model_name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
        "    \n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "    \n",
        "    scaler = GradScaler(enabled=torch.cuda.is_available())\n",
        "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1} [Train]')\n",
        "    \n",
        "    for batch_idx, (inputs, labels) in enumerate(progress_bar):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        \n",
        "        with autocast(enabled=torch.cuda.is_available()):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        total_samples += inputs.size(0)\n",
        "        \n",
        "        current_loss = running_loss / total_samples\n",
        "        current_acc = running_corrects.double() / total_samples\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f'{current_loss:.4f}',\n",
        "            'acc': f'{current_acc:.4f}'\n",
        "        })\n",
        "    \n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = running_corrects.double() / total_samples\n",
        "    \n",
        "    return epoch_loss, epoch_acc.item()\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device, epoch):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "    \n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    \n",
        "    progress_bar = tqdm(val_loader, desc=f'Epoch {epoch+1} [Val]')\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in progress_bar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            with autocast(enabled=torch.cuda.is_available()):\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "            \n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            \n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            total_samples += inputs.size(0)\n",
        "            \n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            current_loss = running_loss / total_samples\n",
        "            current_acc = running_corrects.double() / total_samples\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{current_loss:.4f}',\n",
        "                'acc': f'{current_acc:.4f}'\n",
        "            })\n",
        "    \n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = running_corrects.double() / total_samples\n",
        "    \n",
        "    return epoch_loss, epoch_acc.item(), all_preds, all_labels, all_probs\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, y_prob, class_names):\n",
        "    \n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    \n",
        "    per_class_precision = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
        "    per_class_recall = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
        "    per_class_f1 = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
        "    \n",
        "    try:\n",
        "        y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
        "        if y_true_bin.shape[1] > 1:  \n",
        "            auc_macro = roc_auc_score(y_true_bin, y_prob, average='macro', multi_class='ovr')\n",
        "        else:\n",
        "            auc_macro = 0.0\n",
        "    except:\n",
        "        auc_macro = 0.0\n",
        "    \n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision_macro': precision,\n",
        "        'recall_macro': recall,\n",
        "        'f1_macro': f1,\n",
        "        'auc_macro': auc_macro,\n",
        "        'per_class_precision': per_class_precision.tolist(),\n",
        "        'per_class_recall': per_class_recall.tolist(),\n",
        "        'per_class_f1': per_class_f1.tolist()\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "print(\" Training functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training function defined successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train_model(model_name, data_loaders, num_epochs=20, learning_rate=0.0003, \n",
        "                save_dir='models/', device=device):\n",
        "    \"\"\"Train a model and return training history and final metrics\"\"\"\n",
        "    \n",
        "    print(f\"\\\\n{'='*60}\")\n",
        "    print(f\" Training {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    \n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs('results', exist_ok=True)\n",
        "    \n",
        "    \n",
        "    try:\n",
        "        model = create_model(model_name, num_classes=4, pretrained=True)\n",
        "        model = model.to(device)\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        print(f\" Model parameters: {total_params/1e6:.1f}M\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error creating model {model_name}: {e}\")\n",
        "        return None\n",
        "    \n",
        "    class_weights = data_loaders['class_weights'].to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, \n",
        "                                                   patience=5)\n",
        "    \n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "    \n",
        "    best_val_acc = 0.0\n",
        "    best_model_state = None\n",
        "    patience_counter = 0\n",
        "    early_stopping_patience = 10\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\\\n Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\" Learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "        \n",
        "        \n",
        "        train_loss, train_acc = train_epoch(model, data_loaders['train'], \n",
        "                                          criterion, optimizer, device, epoch)\n",
        "        \n",
        "        \n",
        "        val_loss, val_acc, val_preds, val_labels, val_probs = validate_epoch(\n",
        "            model, data_loaders['val'], criterion, device, epoch)\n",
        "        \n",
        "        \n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        \n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        print(f\" Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\" Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        print(f\" Best Val Acc: {best_val_acc:.4f}\")\n",
        "        \n",
        "        \n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(f\" Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"\\\\n Training completed in {training_time:.2f} seconds\")\n",
        "    \n",
        "    \n",
        "    model.load_state_dict(best_model_state)\n",
        "    \n",
        "    \n",
        "    print(\"Evaluating on test set...\")\n",
        "    test_loss, test_acc, test_preds, test_labels, test_probs = validate_epoch(\n",
        "        model, data_loaders['test'], criterion, device, -1)\n",
        "    \n",
        "    \n",
        "    if 'data_splits' in globals():\n",
        "        class_names = data_splits['class_names']\n",
        "    else:\n",
        "        class_names = ['Non Demented', 'Very mild Dementia', 'Mild Dementia', 'Moderate Dementia']\n",
        "    \n",
        "    test_metrics = calculate_metrics(test_labels, test_preds, test_probs, class_names)\n",
        "    \n",
        "    print(f\" Final Test Results:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "    print(f\"Test F1 (macro): {test_metrics['f1_macro']:.4f}\")\n",
        "    print(f\"Test Precision (macro): {test_metrics['precision_macro']:.4f}\")\n",
        "    print(f\"Test Recall (macro): {test_metrics['recall_macro']:.4f}\")\n",
        "    print(f\"Test AUC (macro): {test_metrics['auc_macro']:.4f}\")\n",
        "    \n",
        "    model_path = os.path.join(save_dir, f\"{model_name}_best.pth\")\n",
        "    torch.save({\n",
        "        'model_state_dict': best_model_state,\n",
        "        'model_name': model_name,\n",
        "        'test_metrics': test_metrics,\n",
        "        'training_history': history,\n",
        "        'training_time': training_time,\n",
        "        'num_epochs': epoch + 1\n",
        "    }, model_path)\n",
        "    \n",
        "    print(f\" Model saved to {model_path}\")\n",
        "    \n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'history': history,\n",
        "        'test_metrics': test_metrics,\n",
        "        'training_time': training_time,\n",
        "        'num_epochs': epoch + 1,\n",
        "        'test_predictions': {\n",
        "            'y_true': test_labels,\n",
        "            'y_pred': test_preds,\n",
        "            'y_prob': test_probs\n",
        "        }\n",
        "    }\n",
        "\n",
        "print(\" Training function defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Visualization functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def plot_training_history(history, model_name):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    \n",
        "    \n",
        "    ax1.plot(epochs, history['train_loss'], 'bo-', label='Training Loss', linewidth=2)\n",
        "    ax1.plot(epochs, history['val_loss'], 'ro-', label='Validation Loss', linewidth=2)\n",
        "    ax1.set_title(f'{model_name} - Training and Validation Loss', fontweight='bold')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    \n",
        "    ax2.plot(epochs, history['train_acc'], 'bo-', label='Training Accuracy', linewidth=2)\n",
        "    ax2.plot(epochs, history['val_acc'], 'ro-', label='Validation Accuracy', linewidth=2)\n",
        "    ax2.set_title(f'{model_name} - Training and Validation Accuracy', fontweight='bold')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names, model_name):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(f'{model_name} - Confusion Matrix', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Predicted', fontsize=12)\n",
        "    plt.ylabel('Actual', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return cm\n",
        "\n",
        "def plot_classification_report_heatmap(y_true, y_pred, class_names, model_name):\n",
        "    \"\"\"Plot classification report as heatmap\"\"\"\n",
        "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "    \n",
        "    \n",
        "    metrics_data = []\n",
        "    for class_name in class_names:\n",
        "        metrics_data.append([\n",
        "            report[class_name]['precision'],\n",
        "            report[class_name]['recall'],\n",
        "            report[class_name]['f1-score']\n",
        "        ])\n",
        "    \n",
        "    df_metrics = pd.DataFrame(metrics_data, \n",
        "                             columns=['Precision', 'Recall', 'F1-Score'],\n",
        "                             index=class_names)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(df_metrics, annot=True, cmap='YlOrRd', fmt='.3f', cbar_kws={'label': 'Score'})\n",
        "    plt.title(f'{model_name} - Classification Report Heatmap', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_curves(y_true, y_prob, class_names, model_name):\n",
        "    \"\"\"Plot ROC curves for each class\"\"\"\n",
        "    \n",
        "    y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
        "    \n",
        "    \n",
        "    if y_true_bin.shape[1] == 1:\n",
        "        y_true_bin = np.hstack([1 - y_true_bin, y_true_bin])\n",
        "        y_prob = np.array(y_prob)\n",
        "        if y_prob.shape[1] == 1:\n",
        "            y_prob = np.hstack([1 - y_prob, y_prob])\n",
        "    \n",
        "    plt.figure(figsize=(12, 10))\n",
        "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
        "    \n",
        "    \n",
        "    for i in range(len(class_names)):\n",
        "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], np.array(y_prob)[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        \n",
        "        plt.plot(fpr, tpr, color=colors[i % len(colors)], linewidth=2,\n",
        "                label=f'{class_names[i]} (AUC = {roc_auc:.3f})')\n",
        "    \n",
        "    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title(f'{model_name} - ROC Curves', fontsize=16, fontweight='bold')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_results(result, class_names):\n",
        "\n",
        "    if result is None:\n",
        "        print(\" No results to visualize\")\n",
        "        return\n",
        "    \n",
        "    model_name = result['model_name']\n",
        "    history = result['history']\n",
        "    predictions = result['test_predictions']\n",
        "    \n",
        "    print(f\"\\\\n Visualizing results for {model_name}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    plot_training_history(history, model_name)\n",
        "    \n",
        "    cm = plot_confusion_matrix(predictions['y_true'], predictions['y_pred'], \n",
        "                              class_names, model_name)\n",
        "    \n",
        "    \n",
        "    plot_classification_report_heatmap(predictions['y_true'], predictions['y_pred'], \n",
        "                                      class_names, model_name)\n",
        "    \n",
        "    \n",
        "    try:\n",
        "        plot_roc_curves(predictions['y_true'], predictions['y_prob'], \n",
        "                       class_names, model_name)\n",
        "    except Exception as e:\n",
        "        print(f\" Could not plot ROC curves: {e}\")\n",
        "\n",
        "print(\" Visualization functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Starting Phase 1 baseline training...\n",
            "  This will take several hours to complete!\n",
            " Starting Phase 1 Training\n",
            " Models to train: ['vgg16', 'vgg19', 'resnet50', 'resnet101', 'resnet152', 'densenet121', 'densenet201', 'mobilenetv3_large', 'shufflenet_v2_x1_0']\n",
            " Epochs per model: 15\n",
            " Device: cuda\n",
            "================================================================================\n",
            " Training Model 1/9: vgg16\n",
            "  Estimated time remaining: 0.0 minutes\n",
            "\\n============================================================\n",
            " Training vgg16\n",
            "============================================================\n",
            " Creating vgg16 model...\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\sajib/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 528M/528M [01:08<00:00, 8.05MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Model parameters: 134.3M\n",
            "\\n Epoch 1/15\n",
            " Learning rate: 0.000300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 [Train]: 100%|██████████| 3782/3782 [18:14<00:00,  3.45it/s, loss=1.3448, acc=0.7686]\n",
            "Epoch 1 [Val]: 100%|██████████| 811/811 [02:40<00:00,  5.07it/s, loss=1.3540, acc=0.7777]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train Loss: 1.3448, Train Acc: 0.7686\n",
            " Val Loss: 1.3540, Val Acc: 0.7777\n",
            " Best Val Acc: 0.7777\n",
            "\\n Epoch 2/15\n",
            " Learning rate: 0.000300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 [Train]: 100%|██████████| 3782/3782 [12:24<00:00,  5.08it/s, loss=1.3672, acc=0.7722]\n",
            "Epoch 2 [Val]: 100%|██████████| 811/811 [00:56<00:00, 14.28it/s, loss=1.3695, acc=0.7777]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Train Loss: 1.3672, Train Acc: 0.7722\n",
            " Val Loss: 1.3695, Val Acc: 0.7777\n",
            " Best Val Acc: 0.7777\n",
            "\\n Epoch 3/15\n",
            " Learning rate: 0.000300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 [Train]:  69%|██████▉   | 2602/3782 [10:20<04:41,  4.20it/s, loss=1.3697, acc=0.7765]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  This will take several hours to complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m models_to_train = [\n\u001b[32m     65\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mvgg16\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     66\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mvgg19\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mshufflenet_v2_x1_0\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     74\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m phase1_results = \u001b[43mrun_phase1_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodels_to_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels_to_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\n\u001b[32m     80\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn Phase 1 Training Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mrun_phase1_training\u001b[39m\u001b[34m(data_loaders, models_to_train, num_epochs)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Estimated time remaining: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m((time.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mtraining_start_time)\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[38;5;28mmax\u001b[39m(i,\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m))\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m(\u001b[38;5;28mlen\u001b[39m(models_to_train)\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mi)\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     result = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0003\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodels/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m     31\u001b[39m         results[model_name] = result\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model_name, data_loaders, num_epochs, learning_rate, save_dir, device)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Learning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer.param_groups[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m train_loss, train_acc = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m val_loss, val_acc, val_preds, val_labels, val_probs = validate_epoch(\n\u001b[32m     57\u001b[39m     model, data_loaders[\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m], criterion, device, epoch)\n\u001b[32m     60\u001b[39m history[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m].append(train_loss)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, device, epoch)\u001b[39m\n\u001b[32m     20\u001b[39m scaler.scale(loss).backward()\n\u001b[32m     21\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m scaler.update()\n\u001b[32m     25\u001b[39m running_loss += loss.item() * inputs.size(\u001b[32m0\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\Code\\Alzheimer-Disease-Diagnosis\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:461\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    455\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    458\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    459\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\Code\\Alzheimer-Disease-Diagnosis\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf_per_device\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\Code\\Alzheimer-Disease-Diagnosis\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "\n",
        "def run_phase1_training(data_loaders, models_to_train=None, num_epochs=20):\n",
        "    \n",
        "    \n",
        "    if models_to_train is None:\n",
        "        models_to_train = PHASE1_MODELS\n",
        "    \n",
        "    print(f\" Starting Phase 1 Training\")\n",
        "    print(f\" Models to train: {models_to_train}\")\n",
        "    print(f\" Epochs per model: {num_epochs}\")\n",
        "    print(f\" Device: {device}\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    results = {}\n",
        "    training_start_time = time.time()\n",
        "    \n",
        "    for i, model_name in enumerate(models_to_train):\n",
        "        print(f\" Training Model {i+1}/{len(models_to_train)}: {model_name}\")\n",
        "        print(f\"  Estimated time remaining: {((time.time() - training_start_time) / max(i, 1)) * (len(models_to_train) - i) / 60:.1f} minutes\")\n",
        "        \n",
        "        try:\n",
        "            result = train_model(\n",
        "                model_name=model_name,\n",
        "                data_loaders=data_loaders,\n",
        "                num_epochs=num_epochs,\n",
        "                learning_rate=0.0003,\n",
        "                save_dir='models/',\n",
        "                device=device\n",
        "            )\n",
        "            \n",
        "            if result:\n",
        "                results[model_name] = result\n",
        "                print(f\" {model_name} training completed successfully!\")\n",
        "            else:\n",
        "                print(f\" {model_name} training failed!\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\" Error training {model_name}: {e}\")\n",
        "            continue\n",
        "        \n",
        "        with open('results/phase1_results.json', 'w') as f:\n",
        "            json_results = {}\n",
        "            for name, res in results.items():\n",
        "                json_results[name] = {\n",
        "                    'model_name': res['model_name'],\n",
        "                    'test_metrics': res['test_metrics'],\n",
        "                    'training_time': res['training_time'],\n",
        "                    'num_epochs': res['num_epochs']\n",
        "                }\n",
        "            json.dump(json_results, f, indent=2)\n",
        "    \n",
        "    total_time = time.time() - training_start_time\n",
        "    print(f\"\\\\n Phase 1 Training Completed!\")\n",
        "    print(f\" Total training time: {total_time/3600:.2f} hours\")\n",
        "    print(f\" Successfully trained: {len(results)}/{len(models_to_train)} models\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "TRAIN_PHASE1 = True  \n",
        "\n",
        "if TRAIN_PHASE1 and 'data_loaders' in locals():\n",
        "    print(\" Starting Phase 1 baseline training...\")\n",
        "    print(\"  This will take several hours to complete!\")\n",
        "    \n",
        "    models_to_train = [\n",
        "        'vgg16',\n",
        "        'vgg19',\n",
        "        'resnet50',\n",
        "        'resnet101',\n",
        "        'resnet152',\n",
        "        'densenet121',\n",
        "        'densenet201',\n",
        "        'mobilenetv3_large',\n",
        "        'shufflenet_v2_x1_0'\n",
        "    ]\n",
        "    \n",
        "    phase1_results = run_phase1_training(\n",
        "        data_loaders=data_loaders,\n",
        "        models_to_train=models_to_train,\n",
        "        num_epochs=15\n",
        "    )\n",
        "    \n",
        "    print(\"\\\\n Phase 1 Training Summary:\")\n",
        "    print(\"=\"*60)\n",
        "    for model_name, result in phase1_results.items():\n",
        "        metrics = result['test_metrics']\n",
        "        print(f\"{model_name}:\")\n",
        "        print(f\"  Test Accuracy: {metrics['accuracy']:.4f}\")\n",
        "        print(f\"  Test F1 (macro): {metrics['f1_macro']:.4f}\")\n",
        "        print(f\"  Training Time: {result['training_time']/60:.1f} min\")\n",
        "        print()\n",
        "        \n",
        "else:\n",
        "    if not 'data_loaders' in locals():\n",
        "        print(\" Data loaders not available. Please run the data loading cells first.\")\n",
        "    else:\n",
        "        print(\" To start Phase 1 training, set TRAIN_PHASE1 = True in the cell above\")\n",
        "        print(\"  Phase 1 training will take several hours to complete!\")\n",
        "        print(\" You can modify the models_to_train list to train specific models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def compare_models(results_dict):\n",
        "    \n",
        "    if not results_dict:\n",
        "        print(\" No results to compare\")\n",
        "        return\n",
        "    \n",
        "    \n",
        "    comparison_data = []\n",
        "    for model_name, result in results_dict.items():\n",
        "        metrics = result['test_metrics']\n",
        "        comparison_data.append({\n",
        "            'Model': model_name,\n",
        "            'Accuracy': metrics['accuracy'],\n",
        "            'F1 (Macro)': metrics['f1_macro'],\n",
        "            'Precision (Macro)': metrics['precision_macro'],\n",
        "            'Recall (Macro)': metrics['recall_macro'],\n",
        "            'AUC (Macro)': metrics['auc_macro'],\n",
        "            'Training Time (min)': result['training_time'] / 60,\n",
        "            'Epochs': result['num_epochs']\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(comparison_data)\n",
        "    df = df.sort_values('Accuracy', ascending=False)\n",
        "    \n",
        "    print(\"Model Performance Comparison\")\n",
        "    print(\"=\"*80)\n",
        "    print(df.round(4))\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    axes[0, 0].bar(df['Model'], df['Accuracy'], color='skyblue')\n",
        "    axes[0, 0].set_title('Test Accuracy Comparison', fontweight='bold')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    \n",
        "    axes[0, 1].bar(df['Model'], df['F1 (Macro)'], color='lightcoral')\n",
        "    axes[0, 1].set_title('F1 Score (Macro) Comparison', fontweight='bold')\n",
        "    axes[0, 1].set_ylabel('F1 Score')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    \n",
        "    axes[1, 0].bar(df['Model'], df['Training Time (min)'], color='lightgreen')\n",
        "    axes[1, 0].set_title('Training Time Comparison', fontweight='bold')\n",
        "    axes[1, 0].set_ylabel('Training Time (minutes)')\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    \n",
        "    metrics_cols = ['Accuracy', 'F1 (Macro)', 'Precision (Macro)', 'Recall (Macro)', 'AUC (Macro)']\n",
        "    df_normalized = df[metrics_cols].copy()\n",
        "    for col in metrics_cols:\n",
        "        df_normalized[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
        "    \n",
        "    for i, (_, row) in enumerate(df.iterrows()):\n",
        "        axes[1, 1].plot(metrics_cols, df_normalized.iloc[i][metrics_cols], \n",
        "                       marker='o', label=row['Model'], linewidth=2)\n",
        "    \n",
        "    axes[1, 1].set_title('Normalized Multi-Metric Comparison', fontweight='bold')\n",
        "    axes[1, 1].set_ylabel('Normalized Score')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "    axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "    best_accuracy = df.loc[df['Accuracy'].idxmax()]\n",
        "    best_f1 = df.loc[df['F1 (Macro)'].idxmax()]\n",
        "    fastest = df.loc[df['Training Time (min)'].idxmin()]\n",
        "    \n",
        "    print(f\" Top Performers:\")\n",
        "    print(f\"Best Accuracy: {best_accuracy['Model']} ({best_accuracy['Accuracy']:.4f})\")\n",
        "    print(f\"Best F1 Score: {best_f1['Model']} ({best_f1['F1 (Macro)']:.4f})\")\n",
        "    print(f\"Fastest Training: {fastest['Model']} ({fastest['Training Time (min)']:.1f} min)\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "def load_saved_results(results_file='results/phase1_results.json'):\n",
        "    \n",
        "    try:\n",
        "        with open(results_file, 'r') as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\" Results file {results_file} not found\")\n",
        "        return {}\n",
        "    except Exception as e:\n",
        "        print(f\" Error loading results: {e}\")\n",
        "        return {}\n",
        "\n",
        "def save_model_comparison_report(df, filename='results/model_comparison_report.csv'):\n",
        "    os.makedirs('results', exist_ok=True)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Model comparison report saved to {filename}\")\n",
        "\n",
        "print(\" Results analysis functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if 'phase1_results' in locals():\n",
        "    print(\" Analyzing Phase 1 training results...\")\n",
        "    comparison_df = compare_models(phase1_results)\n",
        "    if comparison_df is not None:\n",
        "        save_model_comparison_report(comparison_df)\n",
        "        \n",
        "else:\n",
        "    \n",
        "    print(\" Looking for saved results...\")\n",
        "    saved_results = load_saved_results()\n",
        "    \n",
        "    if saved_results:\n",
        "        print(\" Analyzing saved Phase 1 results...\")\n",
        "        comparison_df = compare_models(saved_results)\n",
        "        if comparison_df is not None:\n",
        "            save_model_comparison_report(comparison_df)\n",
        "    else:\n",
        "        print(\" No results available for analysis yet.\")\n",
        "        print(\" After training models, results will be automatically analyzed here.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
