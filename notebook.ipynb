{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import plotly.express as px\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "import kagglehub\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
        "                           f1_score, confusion_matrix, classification_report,\n",
        "                           roc_curve, auc, roc_auc_score)\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "print(\"Basic libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"CUDA not available, using CPU\")\n",
        "\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "print(f\"PyTorch {torch.__version__} setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_dataset_path():\n",
        "    \"\"\"Automatically find the OASIS dataset path\"\"\"\n",
        "    possible_paths = [\n",
        "        '/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1/Data',\n",
        "        '/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1',\n",
        "        'Data',\n",
        "        './Data',\n",
        "        '../Data'\n",
        "    ]\n",
        "    \n",
        "    expected_classes = ['Mild Dementia', 'Very mild Dementia', 'Moderate Dementia', 'Non Demented']\n",
        "    \n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
        "            if any(cls in subdirs for cls in expected_classes):\n",
        "                return path, subdirs\n",
        "    \n",
        "    return None, []\n",
        "dataset_path, found_classes = find_dataset_path()\n",
        "\n",
        "if dataset_path:\n",
        "    print(f\" Dataset found at: {dataset_path}\")\n",
        "    print(f\" Classes found: {found_classes}\")\n",
        "    \n",
        "    total_images = 0\n",
        "    class_counts = {}\n",
        "    \n",
        "    for class_name in found_classes:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            image_files = [f for f in os.listdir(class_dir) \n",
        "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "            count = len(image_files)\n",
        "            class_counts[class_name] = count\n",
        "            total_images += count\n",
        "            print(f\"   {class_name}: {count:,} images\")\n",
        "    \n",
        "    print(f\"Total images: {total_images:,}\")\n",
        "else:\n",
        "    print(\"Dataset not found! Please ensure the OASIS dataset is downloaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if dataset_path and class_counts:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    classes = list(class_counts.keys())\n",
        "    counts = list(class_counts.values())\n",
        "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
        "    \n",
        "    bars = ax1.bar(classes, counts, color=colors)\n",
        "    ax1.set_title('Alzheimer MRI Dataset - Class Distribution', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Dementia Severity')\n",
        "    ax1.set_ylabel('Number of Images')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    for bar, count in zip(bars, counts):\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + max(counts)*0.01,\n",
        "                f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    ax2.pie(counts, labels=classes, autopct='%1.1f%%', startangle=90, colors=colors)\n",
        "    ax2.set_title('Class Proportions', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f\"Dataset Statistics:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Total images: {sum(counts):,}\")\n",
        "    print(f\"Number of classes: {len(classes)}\")\n",
        "    print(f\"Average images per class: {np.mean(counts):,.0f}\")\n",
        "    print(f\"Std deviation: {np.std(counts):,.0f}\")\n",
        "    print(f\"Min images in a class: {min(counts):,}\")\n",
        "    print(f\"Max images in a class: {max(counts):,}\")\n",
        "    \n",
        "    imbalance_ratio = max(counts) / min(counts)\n",
        "    print(f\"Class imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "    \n",
        "    if imbalance_ratio > 5:\n",
        "        print(\" Significant class imbalance detected - will use weighted loss for training\")\n",
        "    else:\n",
        "        print(\"Moderate class imbalance - manageable with weighted loss for training\")\n",
        "else:\n",
        "    print(\"Cannot visualize - dataset not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "possible_paths = [\n",
        "    '/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1/Data',\n",
        "    '/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1',\n",
        "    'Data',\n",
        "    './Data',\n",
        "    '../Data'\n",
        "]\n",
        "\n",
        "dataset_path = None\n",
        "for path in possible_paths:\n",
        "    if os.path.exists(path):\n",
        "        subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
        "        expected_classes = ['Mild Dementia', 'Very mild Dementia', 'Moderate Dementia', 'Non Demented']\n",
        "        \n",
        "        if any(cls in subdirs for cls in expected_classes):\n",
        "            dataset_path = path\n",
        "            print(f\"Dataset found at: {dataset_path}\")\n",
        "            print(f\"Subdirectories: {subdirs}\")\n",
        "            break\n",
        "\n",
        "if dataset_path is None:\n",
        "    print(\"Dataset not found automatically. Please update the dataset_path variable manually.\")\n",
        "    print(\"Expected structure:\")\n",
        "    print(\"Data/\")\n",
        "    print(\"  Mild Dementia/\")\n",
        "    print(\"  Very mild Dementia/\")\n",
        "    print(\"  Moderate Dementia/\")\n",
        "    print(\"  Non Demented/\")\n",
        "else:\n",
        "    print(f\"Using dataset path: {dataset_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def explore_dataset(dataset_path):\n",
        "    \"\"\"Explore the dataset and return information about classes and image counts\"\"\"\n",
        "    \n",
        "    if dataset_path is None:\n",
        "        print(\"Please set the correct dataset_path in the previous cell\")\n",
        "        return None\n",
        "    \n",
        "    class_names = [d for d in os.listdir(dataset_path) \n",
        "                   if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "    \n",
        "    print(f\"Found {len(class_names)} classes:\")\n",
        "    \n",
        "    class_info = {}\n",
        "    total_images = 0\n",
        "    \n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        image_files = [f for f in os.listdir(class_dir) \n",
        "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        \n",
        "        class_info[class_name] = {\n",
        "            'count': len(image_files),\n",
        "            'sample_files': image_files[:3] \n",
        "        }\n",
        "        total_images += len(image_files)\n",
        "        \n",
        "        print(f\"  {class_name}: {len(image_files)} images\")\n",
        "        print(f\"    Sample files: {image_files[:3]}\")\n",
        "    \n",
        "    print(f\"\\nTotal images: {total_images}\")\n",
        "    \n",
        "    return {\n",
        "        'class_names': class_names,\n",
        "        'class_info': class_info,\n",
        "        'total_images': total_images,\n",
        "        'dataset_path': dataset_path\n",
        "    }\n",
        "\n",
        "\n",
        "dataset_info = explore_dataset(dataset_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if dataset_info is not None:\n",
        "    class_names = dataset_info['class_names']\n",
        "    class_counts = [dataset_info['class_info'][name]['count'] for name in class_names]\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    bars = ax1.bar(class_names, class_counts, color=['skyblue', 'lightgreen', 'orange', 'salmon'])\n",
        "    ax1.set_title('Number of Images per Class', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Class')\n",
        "    ax1.set_ylabel('Number of Images')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    for bar, count in zip(bars, class_counts):\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 50,\n",
        "                f'{count}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    \n",
        "    ax2.pie(class_counts, labels=class_names, autopct='%1.1f%%', startangle=140,\n",
        "            colors=['skyblue', 'lightgreen', 'orange', 'salmon'])\n",
        "    ax2.set_title('Class Proportions', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Dataset Statistics:\")\n",
        "    print(f\"Total images: {sum(class_counts):,}\")\n",
        "    print(f\"Number of classes: {len(class_names)}\")\n",
        "    print(f\"Average images per class: {np.mean(class_counts):.1f}\")\n",
        "    print(f\"Min images in a class: {min(class_counts)}\")\n",
        "    print(f\"Max images in a class: {max(class_counts)}\")\n",
        "    \n",
        "    imbalance_ratio = max(class_counts) / min(class_counts)\n",
        "    print(f\"Class imbalance ratio: {imbalance_ratio:.2f}\")\n",
        "else:\n",
        "    print(\"Please fix the dataset path issue first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def display_sample_images(dataset_info, samples_per_class=3):\n",
        "    if dataset_info is None:\n",
        "        print(\"Dataset info not available\")\n",
        "        return\n",
        "    \n",
        "    class_names = dataset_info['class_names']\n",
        "    dataset_path = dataset_info['dataset_path']\n",
        "    \n",
        "    fig, axes = plt.subplots(len(class_names), samples_per_class, \n",
        "                            figsize=(samples_per_class * 4, len(class_names) * 3))\n",
        "    \n",
        "    if len(class_names) == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    \n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        image_files = [f for f in os.listdir(class_dir) \n",
        "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        \n",
        "        sample_files = random.sample(image_files, min(samples_per_class, len(image_files)))\n",
        "        \n",
        "        for j, img_file in enumerate(sample_files):\n",
        "            img_path = os.path.join(class_dir, img_file)\n",
        "            try:\n",
        "                img = Image.open(img_path)\n",
        "                axes[i, j].imshow(img, cmap='gray')\n",
        "                axes[i, j].set_title(f\"{class_name}\\\\n{img_file}\", fontsize=10)\n",
        "                axes[i, j].axis('off')\n",
        "            except Exception as e:\n",
        "                axes[i, j].text(0.5, 0.5, f\"Error loading\\\\n{img_file}\", \n",
        "                               ha='center', va='center', transform=axes[i, j].transAxes)\n",
        "                axes[i, j].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if dataset_info is not None:\n",
        "    print(\"Sample images from each class:\")\n",
        "    display_sample_images(dataset_info, samples_per_class=4)\n",
        "else:\n",
        "    print(\"Please fix the dataset path first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_image_properties(dataset_info, sample_size=100):\n",
        "    \"\"\"Analyze image properties like size, format, etc.\"\"\"\n",
        "    \n",
        "    if dataset_info is None:\n",
        "        print(\"Dataset info not available\")\n",
        "        return None\n",
        "    \n",
        "    class_names = dataset_info['class_names']\n",
        "    dataset_path = dataset_info['dataset_path']\n",
        "    \n",
        "    image_properties = []\n",
        "    \n",
        "    print(\"Analyzing image properties...\")\n",
        "    \n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        image_files = [f for f in os.listdir(class_dir) \n",
        "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        \n",
        "        sample_files = random.sample(image_files, min(sample_size, len(image_files)))\n",
        "        \n",
        "        for img_file in sample_files:\n",
        "            img_path = os.path.join(class_dir, img_file)\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    properties = {\n",
        "                        'class': class_name,\n",
        "                        'filename': img_file,\n",
        "                        'width': img.width,\n",
        "                        'height': img.height,\n",
        "                        'mode': img.mode,\n",
        "                        'format': img.format\n",
        "                    }\n",
        "                    image_properties.append(properties)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {img_path}: {e}\")\n",
        "    \n",
        "    df = pd.DataFrame(image_properties)\n",
        "    \n",
        "    print(f\"\\\\nAnalyzed {len(df)} images\")\n",
        "    print(f\"Image formats found: {df['format'].unique()}\")\n",
        "    print(f\"Image modes found: {df['mode'].unique()}\")\n",
        "    \n",
        "    print(f\"\\\\nImage size statistics:\")\n",
        "    print(f\"Width - Min: {df['width'].min()}, Max: {df['width'].max()}, Mean: {df['width'].mean():.1f}\")\n",
        "    print(f\"Height - Min: {df['height'].min()}, Max: {df['height'].max()}, Mean: {df['height'].mean():.1f}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "if dataset_info is not None:\n",
        "    image_props_df = analyze_image_properties(dataset_info, sample_size=50)\n",
        "else:\n",
        "    print(\"Please fix the dataset path first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if 'image_props_df' in locals() and image_props_df is not None:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    for class_name in image_props_df['class'].unique():\n",
        "        class_data = image_props_df[image_props_df['class'] == class_name]\n",
        "        axes[0, 0].hist(class_data['width'], alpha=0.7, label=class_name, bins=20)\n",
        "    axes[0, 0].set_title('Image Width Distribution by Class')\n",
        "    axes[0, 0].set_xlabel('Width (pixels)')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "    axes[0, 0].legend()\n",
        "    \n",
        "    for class_name in image_props_df['class'].unique():\n",
        "        class_data = image_props_df[image_props_df['class'] == class_name]\n",
        "        axes[0, 1].hist(class_data['height'], alpha=0.7, label=class_name, bins=20)\n",
        "    axes[0, 1].set_title('Image Height Distribution by Class')\n",
        "    axes[0, 1].set_xlabel('Height (pixels)')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    axes[0, 1].legend()\n",
        "    \n",
        "    sns.boxplot(data=image_props_df, x='class', y='width', ax=axes[1, 0])\n",
        "    axes[1, 0].set_title('Width Distribution by Class (Boxplot)')\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    sns.boxplot(data=image_props_df, x='class', y='height', ax=axes[1, 1])\n",
        "    axes[1, 1].set_title('Height Distribution by Class (Boxplot)')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    unique_sizes = image_props_df[['width', 'height']].drop_duplicates()\n",
        "    print(f\"Number of unique image sizes: {len(unique_sizes)}\")\n",
        "    if len(unique_sizes) <= 5:\n",
        "        print(\"Most common image sizes:\")\n",
        "        size_counts = image_props_df.groupby(['width', 'height']).size().sort_values(ascending=False)\n",
        "        print(size_counts.head())\n",
        "    else:\n",
        "        print(\"Images have varying sizes - resizing will be needed for model training\")\n",
        "else:\n",
        "    print(\"Image properties data not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = kagglehub.dataset_download(\"ninadaithal/imagesoasis\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "dataset_path = \"/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1/Data\"\n",
        "\n",
        "classes = ['Non Demented', 'Very mild Dementia', 'Mild Dementia', 'Moderate Dementia']\n",
        "class_counts = {cls: len(glob.glob(f\"{dataset_path}/{cls}/*.jpg\")) for cls in classes}\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))\n",
        "plt.title('Alzheimer MRI Class Distribution')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "dimension_counts = {cls: {} for cls in classes}\n",
        "\n",
        "for cls in classes:\n",
        "    cls_path = os.path.join(dataset_path, cls)\n",
        "    image_files = glob.glob(os.path.join(cls_path, '*.jpg'))\n",
        "    \n",
        "    for img_file in image_files:\n",
        "        with Image.open(img_file) as img:\n",
        "            dimensions = img.size  \n",
        "            if dimensions in dimension_counts[cls]:\n",
        "                dimension_counts[cls][dimensions] += 1\n",
        "            else:\n",
        "                dimension_counts[cls][dimensions] = 1\n",
        "\n",
        "\n",
        "x_labels = list(classes)\n",
        "x_pos = np.arange(len(x_labels))\n",
        "y_labels = set()\n",
        "for cls in classes:\n",
        "    y_labels.update(dimension_counts[cls].keys())\n",
        "y_labels = sorted(y_labels)\n",
        "y_pos = np.arange(len(y_labels))\n",
        "\n",
        "x, y = np.meshgrid(x_pos, y_pos)\n",
        "z = np.zeros_like(x)\n",
        "\n",
        "\n",
        "for i, cls in enumerate(classes):\n",
        "    for j, dim in enumerate(y_labels):\n",
        "        z[j, i] = dimension_counts[cls].get(dim, 0)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "\n",
        "dx = dy = 0.5\n",
        "dz = z.flatten()\n",
        "ax.bar3d(x.flatten(), y.flatten(), np.zeros_like(dz), dx, dy, dz, shade=True)\n",
        "\n",
        "\n",
        "ax.set_xlabel('Class')\n",
        "ax.set_ylabel('Image Dimensions (Width x Height)')\n",
        "ax.set_zlabel('Count')\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels([f'{dim[0]}x{dim[1]}' for dim in y_labels], rotation=45, ha='right')\n",
        "\n",
        "plt.title('3D Bar Chart: Count vs Class vs Image Dimensions')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_data = {\n",
        "    'Class': list(class_counts.keys()),\n",
        "    'Count': list(class_counts.values())\n",
        "}\n",
        "\n",
        "df_class = pd.DataFrame(class_data)\n",
        "\n",
        "fig = px.treemap(\n",
        "    df_class,\n",
        "    path=['Class'],\n",
        "    values='Count',\n",
        "    title='Treemap: Class Proportions',\n",
        "    color='Class',\n",
        "    color_discrete_sequence=px.colors.qualitative.Pastel\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_path = '/Users/sajibhossain/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1/Data'\n",
        "\n",
        "class_names = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "print(\"Classes found:\", class_names)\n",
        "\n",
        "\n",
        "for class_name in class_names:\n",
        "    class_dir = os.path.join(dataset_path, class_name)\n",
        "    num_images = len([f for f in os.listdir(class_dir) if f.endswith('.jpg')])\n",
        "    print(f\"{class_name}: {num_images} images\")\n",
        "\n",
        "fig, axes = plt.subplots(len(class_names), 3, figsize=(12, 3 * len(class_names)))\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_dir = os.path.join(dataset_path, class_name)\n",
        "    image_files = [f for f in os.listdir(class_dir) if f.endswith('.jpg')]\n",
        "    for j in range(3):\n",
        "        img_path = os.path.join(class_dir, image_files[j])\n",
        "        img = Image.open(img_path)\n",
        "        axes[i, j].imshow(img, cmap='gray')\n",
        "        axes[i, j].set_title(f\"{class_name}\\n{image_files[j]}\")\n",
        "        axes[i, j].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "image_shapes = []\n",
        "for class_name in class_names:\n",
        "    class_dir = os.path.join(dataset_path, class_name)\n",
        "    for fname in os.listdir(class_dir):\n",
        "        if fname.endswith('.jpg'):\n",
        "            img = Image.open(os.path.join(class_dir, fname))\n",
        "            image_shapes.append(img.size)\n",
        "\n",
        "\n",
        "widths, heights = zip(*image_shapes)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(widths, bins=20, color='orange')\n",
        "plt.title('Image Width Distribution')\n",
        "plt.xlabel('Width (pixels)')\n",
        "plt.ylabel('Count')\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist(heights, bins=20, color='green')\n",
        "plt.title('Image Height Distribution')\n",
        "plt.xlabel('Height (pixels)')\n",
        "plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "for class_name in class_names:\n",
        "    class_dir = os.path.join(dataset_path, class_name)\n",
        "    pixels = []\n",
        "    for fname in os.listdir(class_dir)[:10]:  \n",
        "        img = Image.open(os.path.join(class_dir, fname)).convert('L')\n",
        "        pixels.extend(np.array(img).flatten())\n",
        "    plt.hist(pixels, bins=50, alpha=0.5, label=class_name)\n",
        "plt.title('Pixel Intensity Distribution by Class')\n",
        "plt.xlabel('Pixel Intensity')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class AlzheimerDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        \n",
        "        self.class_to_idx = {\n",
        "            'Non Demented': 0,\n",
        "            'Very mild Dementia': 1,\n",
        "            'Mild Dementia': 2,\n",
        "            'Moderate Dementia': 3\n",
        "        }\n",
        "        \n",
        "        if len(self.labels) > 0 and isinstance(self.labels[0], str):\n",
        "            self.labels = [self.class_to_idx[label] for label in self.labels]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n",
        "\n",
        "def get_transforms(img_size=224, is_training=False):\n",
        "    \"\"\"Get data transforms - using only torchvision to avoid multiprocessing issues\"\"\"\n",
        "    \n",
        "    if is_training:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomVerticalFlip(p=0.2),\n",
        "            transforms.RandomRotation(15),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.1),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "print(\" Dataset class and transforms defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def load_and_split_data(dataset_path, class_names, test_size=0.15, val_size=0.15, random_state=42):\n",
        "    \"\"\"Load dataset and create train/val/test splits (70-15-15)\"\"\"\n",
        "    \n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    \n",
        "    print(f\" Loading data from {len(class_names)} classes...\")\n",
        "    \n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        if not os.path.exists(class_dir):\n",
        "            continue\n",
        "            \n",
        "        image_files = [f for f in os.listdir(class_dir) \n",
        "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        \n",
        "        for img_file in image_files:\n",
        "            image_paths.append(os.path.join(class_dir, img_file))\n",
        "            labels.append(class_name)\n",
        "    \n",
        "    print(f\" Total images loaded: {len(image_paths):,}\")\n",
        "    \n",
        "    class_dist = Counter(labels)\n",
        "    for class_name, count in class_dist.items():\n",
        "        print(f\"   {class_name}: {count:,} images ({count/len(labels)*100:.1f}%)\")\n",
        "    \n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        image_paths, labels, \n",
        "        test_size=test_size, \n",
        "        stratify=labels, \n",
        "        random_state=random_state\n",
        "    )\n",
        "    \n",
        "    val_size_adjusted = val_size / (1 - test_size)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, \n",
        "        test_size=val_size_adjusted, \n",
        "        stratify=y_temp, \n",
        "        random_state=random_state\n",
        "    )\n",
        "    \n",
        "    print(f\"\\\\n Dataset split completed:\")\n",
        "    print(f\"   Train: {len(X_train):,} images ({len(X_train)/len(image_paths)*100:.1f}%)\")\n",
        "    print(f\"   Validation: {len(X_val):,} images ({len(X_val)/len(image_paths)*100:.1f}%)\")\n",
        "    print(f\"   Test: {len(X_test):,} images ({len(X_test)/len(image_paths)*100:.1f}%)\")\n",
        "    \n",
        "    return {\n",
        "        'train': {'paths': X_train, 'labels': y_train},\n",
        "        'val': {'paths': X_val, 'labels': y_val},\n",
        "        'test': {'paths': X_test, 'labels': y_test},\n",
        "        'class_names': class_names,\n",
        "        'class_counts': class_dist\n",
        "    }\n",
        "\n",
        "def create_data_loaders(data_splits, batch_size=32, img_size=224, num_workers=0):\n",
        "    \"\"\"Create data loaders (num_workers=0 to avoid multiprocessing issues)\"\"\"\n",
        "    \n",
        "    train_transform = get_transforms(img_size=img_size, is_training=True)\n",
        "    val_transform = get_transforms(img_size=img_size, is_training=False)\n",
        "    \n",
        "    train_dataset = AlzheimerDataset(\n",
        "        data_splits['train']['paths'],\n",
        "        data_splits['train']['labels'],\n",
        "        transform=train_transform\n",
        "    )\n",
        "    \n",
        "    val_dataset = AlzheimerDataset(\n",
        "        data_splits['val']['paths'],\n",
        "        data_splits['val']['labels'],\n",
        "        transform=val_transform\n",
        "    )\n",
        "    \n",
        "    test_dataset = AlzheimerDataset(\n",
        "        data_splits['test']['paths'],\n",
        "        data_splits['test']['labels'],\n",
        "        transform=val_transform\n",
        "    )\n",
        "    \n",
        "    train_labels_int = [train_dataset.class_to_idx[label] for label in data_splits['train']['labels']]\n",
        "    class_weights = compute_class_weight(\n",
        "        'balanced', \n",
        "        classes=np.unique(train_labels_int), \n",
        "        y=train_labels_int\n",
        "    )\n",
        "    class_weights = torch.FloatTensor(class_weights)\n",
        "    \n",
        "    print(f\" Class weights: {class_weights.numpy()}\")\n",
        "    \n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True, \n",
        "        num_workers=num_workers, pin_memory=False\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=False, \n",
        "        num_workers=num_workers, pin_memory=False\n",
        "    )\n",
        "    \n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=False, \n",
        "        num_workers=num_workers, pin_memory=False\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'train': train_loader,\n",
        "        'val': val_loader,\n",
        "        'test': test_loader,\n",
        "        'class_weights': class_weights\n",
        "    }\n",
        "\n",
        "if dataset_path and found_classes:\n",
        "    print(\" Loading and splitting dataset...\")\n",
        "    data_splits = load_and_split_data(dataset_path, found_classes)\n",
        "    print(\" Creating data loaders...\")\n",
        "    data_loaders = create_data_loaders(data_splits, batch_size=16, img_size=224)\n",
        "    print(\" Data loaders created successfully!\")\n",
        "else:\n",
        "    print(\"Cannot create data loaders - dataset not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'data_loaders' in locals():\n",
        "    print(\" Testing data loaders...\")\n",
        "    \n",
        "    for phase in ['train', 'val', 'test']:\n",
        "        loader = data_loaders[phase]\n",
        "        \n",
        "        try:\n",
        "            for images, labels in loader:\n",
        "                print(f\" {phase.capitalize()}: Batch shape {images.shape}, Labels shape {labels.shape}\")\n",
        "                print(f\"   Label range: {labels.min().item()}-{labels.max().item()}\")\n",
        "                print(f\"   Image value range: {images.min().item():.3f} to {images.max().item():.3f}\")\n",
        "                break\n",
        "        except Exception as e:\n",
        "            print(f\" Error in {phase} loader: {e}\")\n",
        "    \n",
        "    print(\"\\\\n All data loaders working correctly!\")\n",
        "else:\n",
        "    print(\" Data loaders not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model(model_name, num_classes=4, pretrained=True):\n",
        "    \n",
        "    print(f\" Creating {model_name} model...\")\n",
        "    \n",
        "    if model_name.startswith('vgg'):\n",
        "        if model_name == 'vgg16':\n",
        "            model = models.vgg16(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "            model.classifier[6] = nn.Linear(4096, num_classes)\n",
        "        elif model_name == 'vgg19':\n",
        "            model = models.vgg19(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "            model.classifier[6] = nn.Linear(4096, num_classes)\n",
        "            \n",
        "    elif model_name.startswith('resnet'):\n",
        "        if model_name == 'resnet50':\n",
        "            model = models.resnet50(weights='IMAGENET1K_V2' if pretrained else None)\n",
        "        elif model_name == 'resnet101':\n",
        "            model = models.resnet101(weights='IMAGENET1K_V2' if pretrained else None)\n",
        "        elif model_name == 'resnet152':\n",
        "            model = models.resnet152(weights='IMAGENET1K_V2' if pretrained else None)\n",
        "        \n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "        \n",
        "    elif model_name.startswith('densenet'):\n",
        "        if model_name == 'densenet121':\n",
        "            model = models.densenet121(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "        elif model_name == 'densenet201':\n",
        "            model = models.densenet201(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "        \n",
        "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
        "        \n",
        "    elif model_name == 'mobilenetv3_large':\n",
        "        model = models.mobilenet_v3_large(weights='IMAGENET1K_V2' if pretrained else None)\n",
        "        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
        "        \n",
        "    elif model_name == 'shufflenet_v2_x1_0':\n",
        "        model = models.shufflenet_v2_x1_0(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "        \n",
        "    else:\n",
        "        raise ValueError(f\"Model {model_name} not supported\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "PHASE1_MODELS = [\n",
        "    'vgg16',\n",
        "    'vgg19', \n",
        "    'resnet50',\n",
        "    'resnet101',\n",
        "    'resnet152',\n",
        "    'densenet121',\n",
        "    'densenet201',\n",
        "    'mobilenetv3_large',\n",
        "    'shufflenet_v2_x1_0'\n",
        "]\n",
        "\n",
        "print(f\" Phase 1 models to train: {PHASE1_MODELS}\")\n",
        "\n",
        "print(\"\\\\n Testing model creation...\")\n",
        "for model_name in PHASE1_MODELS[:8]: \n",
        "    try:\n",
        "        model = create_model(model_name, num_classes=4, pretrained=False)\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        print(f\" {model_name}: {total_params/1e6:.1f}M total params, {trainable_params/1e6:.1f}M trainable\")\n",
        "        del model\n",
        "    except Exception as e:\n",
        "        print(f\" Error creating {model_name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
        "    \n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "    \n",
        "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1} [Train]')\n",
        "    \n",
        "    for batch_idx, (inputs, labels) in enumerate(progress_bar):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        total_samples += inputs.size(0)\n",
        "        \n",
        "        current_loss = running_loss / total_samples\n",
        "        current_acc = running_corrects.double() / total_samples\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f'{current_loss:.4f}',\n",
        "            'acc': f'{current_acc:.4f}'\n",
        "        })\n",
        "    \n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = running_corrects.double() / total_samples\n",
        "    \n",
        "    return epoch_loss, epoch_acc.item()\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device, epoch):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "    \n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    \n",
        "    progress_bar = tqdm(val_loader, desc=f'Epoch {epoch+1} [Val]')\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in progress_bar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            \n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            total_samples += inputs.size(0)\n",
        "            \n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            current_loss = running_loss / total_samples\n",
        "            current_acc = running_corrects.double() / total_samples\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{current_loss:.4f}',\n",
        "                'acc': f'{current_acc:.4f}'\n",
        "            })\n",
        "    \n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = running_corrects.double() / total_samples\n",
        "    \n",
        "    return epoch_loss, epoch_acc.item(), all_preds, all_labels, all_probs\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, y_prob, class_names):\n",
        "    \n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    \n",
        "    per_class_precision = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
        "    per_class_recall = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
        "    per_class_f1 = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
        "    \n",
        "    try:\n",
        "        y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
        "        if y_true_bin.shape[1] > 1:  \n",
        "            auc_macro = roc_auc_score(y_true_bin, y_prob, average='macro', multi_class='ovr')\n",
        "        else:\n",
        "            auc_macro = 0.0\n",
        "    except:\n",
        "        auc_macro = 0.0\n",
        "    \n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision_macro': precision,\n",
        "        'recall_macro': recall,\n",
        "        'f1_macro': f1,\n",
        "        'auc_macro': auc_macro,\n",
        "        'per_class_precision': per_class_precision.tolist(),\n",
        "        'per_class_recall': per_class_recall.tolist(),\n",
        "        'per_class_f1': per_class_f1.tolist()\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "print(\" Training functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def train_model(model_name, data_loaders, num_epochs=20, learning_rate=0.001, \n",
        "                save_dir='models/', device=device):\n",
        "    \"\"\"Train a model and return training history and final metrics\"\"\"\n",
        "    \n",
        "    print(f\"\\\\n{'='*60}\")\n",
        "    print(f\" Training {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    \n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs('results', exist_ok=True)\n",
        "    \n",
        "    \n",
        "    try:\n",
        "        model = create_model(model_name, num_classes=4, pretrained=False)\n",
        "        model = model.to(device)\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        print(f\" Model parameters: {total_params/1e6:.1f}M\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error creating model {model_name}: {e}\")\n",
        "        return None\n",
        "    \n",
        "    class_weights = data_loaders['class_weights'].to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, \n",
        "                                                   patience=5)\n",
        "    \n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "    \n",
        "    best_val_acc = 0.0\n",
        "    best_model_state = None\n",
        "    patience_counter = 0\n",
        "    early_stopping_patience = 10\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\\\n Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\" Learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "        \n",
        "        \n",
        "        train_loss, train_acc = train_epoch(model, data_loaders['train'], \n",
        "                                          criterion, optimizer, device, epoch)\n",
        "        \n",
        "        \n",
        "        val_loss, val_acc, val_preds, val_labels, val_probs = validate_epoch(\n",
        "            model, data_loaders['val'], criterion, device, epoch)\n",
        "        \n",
        "        \n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        \n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        print(f\" Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\" Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        print(f\" Best Val Acc: {best_val_acc:.4f}\")\n",
        "        \n",
        "        \n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(f\" Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"\\\\n Training completed in {training_time:.2f} seconds\")\n",
        "    \n",
        "    \n",
        "    model.load_state_dict(best_model_state)\n",
        "    \n",
        "    \n",
        "    print(\"Evaluating on test set...\")\n",
        "    test_loss, test_acc, test_preds, test_labels, test_probs = validate_epoch(\n",
        "        model, data_loaders['test'], criterion, device, -1)\n",
        "    \n",
        "    \n",
        "    if 'data_splits' in globals():\n",
        "        class_names = data_splits['class_names']\n",
        "    else:\n",
        "        class_names = ['Non Demented', 'Very mild Dementia', 'Mild Dementia', 'Moderate Dementia']\n",
        "    \n",
        "    test_metrics = calculate_metrics(test_labels, test_preds, test_probs, class_names)\n",
        "    \n",
        "    print(f\" Final Test Results:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "    print(f\"Test F1 (macro): {test_metrics['f1_macro']:.4f}\")\n",
        "    print(f\"Test Precision (macro): {test_metrics['precision_macro']:.4f}\")\n",
        "    print(f\"Test Recall (macro): {test_metrics['recall_macro']:.4f}\")\n",
        "    print(f\"Test AUC (macro): {test_metrics['auc_macro']:.4f}\")\n",
        "    \n",
        "    model_path = os.path.join(save_dir, f\"{model_name}_best.pth\")\n",
        "    torch.save({\n",
        "        'model_state_dict': best_model_state,\n",
        "        'model_name': model_name,\n",
        "        'test_metrics': test_metrics,\n",
        "        'training_history': history,\n",
        "        'training_time': training_time,\n",
        "        'num_epochs': epoch + 1\n",
        "    }, model_path)\n",
        "    \n",
        "    print(f\" Model saved to {model_path}\")\n",
        "    \n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'history': history,\n",
        "        'test_metrics': test_metrics,\n",
        "        'training_time': training_time,\n",
        "        'num_epochs': epoch + 1,\n",
        "        'test_predictions': {\n",
        "            'y_true': test_labels,\n",
        "            'y_pred': test_preds,\n",
        "            'y_prob': test_probs\n",
        "        }\n",
        "    }\n",
        "\n",
        "print(\" Training function defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def plot_training_history(history, model_name):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    \n",
        "    \n",
        "    ax1.plot(epochs, history['train_loss'], 'bo-', label='Training Loss', linewidth=2)\n",
        "    ax1.plot(epochs, history['val_loss'], 'ro-', label='Validation Loss', linewidth=2)\n",
        "    ax1.set_title(f'{model_name} - Training and Validation Loss', fontweight='bold')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    \n",
        "    ax2.plot(epochs, history['train_acc'], 'bo-', label='Training Accuracy', linewidth=2)\n",
        "    ax2.plot(epochs, history['val_acc'], 'ro-', label='Validation Accuracy', linewidth=2)\n",
        "    ax2.set_title(f'{model_name} - Training and Validation Accuracy', fontweight='bold')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names, model_name):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(f'{model_name} - Confusion Matrix', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Predicted', fontsize=12)\n",
        "    plt.ylabel('Actual', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return cm\n",
        "\n",
        "def plot_classification_report_heatmap(y_true, y_pred, class_names, model_name):\n",
        "    \"\"\"Plot classification report as heatmap\"\"\"\n",
        "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "    \n",
        "    \n",
        "    metrics_data = []\n",
        "    for class_name in class_names:\n",
        "        metrics_data.append([\n",
        "            report[class_name]['precision'],\n",
        "            report[class_name]['recall'],\n",
        "            report[class_name]['f1-score']\n",
        "        ])\n",
        "    \n",
        "    df_metrics = pd.DataFrame(metrics_data, \n",
        "                             columns=['Precision', 'Recall', 'F1-Score'],\n",
        "                             index=class_names)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(df_metrics, annot=True, cmap='YlOrRd', fmt='.3f', cbar_kws={'label': 'Score'})\n",
        "    plt.title(f'{model_name} - Classification Report Heatmap', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_curves(y_true, y_prob, class_names, model_name):\n",
        "    \"\"\"Plot ROC curves for each class\"\"\"\n",
        "    \n",
        "    y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
        "    \n",
        "    \n",
        "    if y_true_bin.shape[1] == 1:\n",
        "        y_true_bin = np.hstack([1 - y_true_bin, y_true_bin])\n",
        "        y_prob = np.array(y_prob)\n",
        "        if y_prob.shape[1] == 1:\n",
        "            y_prob = np.hstack([1 - y_prob, y_prob])\n",
        "    \n",
        "    plt.figure(figsize=(12, 10))\n",
        "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
        "    \n",
        "    \n",
        "    for i in range(len(class_names)):\n",
        "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], np.array(y_prob)[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        \n",
        "        plt.plot(fpr, tpr, color=colors[i % len(colors)], linewidth=2,\n",
        "                label=f'{class_names[i]} (AUC = {roc_auc:.3f})')\n",
        "    \n",
        "    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title(f'{model_name} - ROC Curves', fontsize=16, fontweight='bold')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_results(result, class_names):\n",
        "\n",
        "    if result is None:\n",
        "        print(\" No results to visualize\")\n",
        "        return\n",
        "    \n",
        "    model_name = result['model_name']\n",
        "    history = result['history']\n",
        "    predictions = result['test_predictions']\n",
        "    \n",
        "    print(f\"\\\\n Visualizing results for {model_name}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    plot_training_history(history, model_name)\n",
        "    \n",
        "    cm = plot_confusion_matrix(predictions['y_true'], predictions['y_pred'], \n",
        "                              class_names, model_name)\n",
        "    \n",
        "    \n",
        "    plot_classification_report_heatmap(predictions['y_true'], predictions['y_pred'], \n",
        "                                      class_names, model_name)\n",
        "    \n",
        "    \n",
        "    try:\n",
        "        plot_roc_curves(predictions['y_true'], predictions['y_prob'], \n",
        "                       class_names, model_name)\n",
        "    except Exception as e:\n",
        "        print(f\" Could not plot ROC curves: {e}\")\n",
        "\n",
        "print(\" Visualization functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def run_phase1_training(data_loaders, models_to_train=None, num_epochs=20):\n",
        "    \n",
        "    \n",
        "    if models_to_train is None:\n",
        "        models_to_train = PHASE1_MODELS\n",
        "    \n",
        "    print(f\" Starting Phase 1 Training\")\n",
        "    print(f\" Models to train: {models_to_train}\")\n",
        "    print(f\" Epochs per model: {num_epochs}\")\n",
        "    print(f\" Device: {device}\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    results = {}\n",
        "    training_start_time = time.time()\n",
        "    \n",
        "    for i, model_name in enumerate(models_to_train):\n",
        "        print(f\" Training Model {i+1}/{len(models_to_train)}: {model_name}\")\n",
        "        print(f\"  Estimated time remaining: {((time.time() - training_start_time) / max(i, 1)) * (len(models_to_train) - i) / 60:.1f} minutes\")\n",
        "        \n",
        "        try:\n",
        "            result = train_model(\n",
        "                model_name=model_name,\n",
        "                data_loaders=data_loaders,\n",
        "                num_epochs=num_epochs,\n",
        "                learning_rate=0.001,\n",
        "                save_dir='models/',\n",
        "                device=device\n",
        "            )\n",
        "            \n",
        "            if result:\n",
        "                results[model_name] = result\n",
        "                print(f\" {model_name} training completed successfully!\")\n",
        "            else:\n",
        "                print(f\" {model_name} training failed!\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\" Error training {model_name}: {e}\")\n",
        "            continue\n",
        "        \n",
        "        with open('results/phase1_results.json', 'w') as f:\n",
        "            json_results = {}\n",
        "            for name, res in results.items():\n",
        "                json_results[name] = {\n",
        "                    'model_name': res['model_name'],\n",
        "                    'test_metrics': res['test_metrics'],\n",
        "                    'training_time': res['training_time'],\n",
        "                    'num_epochs': res['num_epochs']\n",
        "                }\n",
        "            json.dump(json_results, f, indent=2)\n",
        "    \n",
        "    total_time = time.time() - training_start_time\n",
        "    print(f\"\\\\n Phase 1 Training Completed!\")\n",
        "    print(f\" Total training time: {total_time/3600:.2f} hours\")\n",
        "    print(f\" Successfully trained: {len(results)}/{len(models_to_train)} models\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "TRAIN_PHASE1 = True  \n",
        "\n",
        "if TRAIN_PHASE1 and 'data_loaders' in locals():\n",
        "    print(\" Starting Phase 1 baseline training...\")\n",
        "    print(\"  This will take several hours to complete!\")\n",
        "    \n",
        "    models_to_train = [\n",
        "        'vgg16',\n",
        "        'vgg19',\n",
        "        'resnet50',\n",
        "        'resnet101',\n",
        "        'resnet152',\n",
        "        'densenet121',\n",
        "        'densenet201',\n",
        "        'mobilenetv3_large',\n",
        "        'shufflenet_v2_x1_0'\n",
        "    ]\n",
        "    \n",
        "    phase1_results = run_phase1_training(\n",
        "        data_loaders=data_loaders,\n",
        "        models_to_train=models_to_train,\n",
        "        num_epochs=15\n",
        "    )\n",
        "    \n",
        "    print(\"\\\\n Phase 1 Training Summary:\")\n",
        "    print(\"=\"*60)\n",
        "    for model_name, result in phase1_results.items():\n",
        "        metrics = result['test_metrics']\n",
        "        print(f\"{model_name}:\")\n",
        "        print(f\"  Test Accuracy: {metrics['accuracy']:.4f}\")\n",
        "        print(f\"  Test F1 (macro): {metrics['f1_macro']:.4f}\")\n",
        "        print(f\"  Training Time: {result['training_time']/60:.1f} min\")\n",
        "        print()\n",
        "        \n",
        "else:\n",
        "    if not 'data_loaders' in locals():\n",
        "        print(\" Data loaders not available. Please run the data loading cells first.\")\n",
        "    else:\n",
        "        print(\" To start Phase 1 training, set TRAIN_PHASE1 = True in the cell above\")\n",
        "        print(\"  Phase 1 training will take several hours to complete!\")\n",
        "        print(\" You can modify the models_to_train list to train specific models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def compare_models(results_dict):\n",
        "    \n",
        "    if not results_dict:\n",
        "        print(\" No results to compare\")\n",
        "        return\n",
        "    \n",
        "    \n",
        "    comparison_data = []\n",
        "    for model_name, result in results_dict.items():\n",
        "        metrics = result['test_metrics']\n",
        "        comparison_data.append({\n",
        "            'Model': model_name,\n",
        "            'Accuracy': metrics['accuracy'],\n",
        "            'F1 (Macro)': metrics['f1_macro'],\n",
        "            'Precision (Macro)': metrics['precision_macro'],\n",
        "            'Recall (Macro)': metrics['recall_macro'],\n",
        "            'AUC (Macro)': metrics['auc_macro'],\n",
        "            'Training Time (min)': result['training_time'] / 60,\n",
        "            'Epochs': result['num_epochs']\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(comparison_data)\n",
        "    df = df.sort_values('Accuracy', ascending=False)\n",
        "    \n",
        "    print(\"Model Performance Comparison\")\n",
        "    print(\"=\"*80)\n",
        "    print(df.round(4))\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    axes[0, 0].bar(df['Model'], df['Accuracy'], color='skyblue')\n",
        "    axes[0, 0].set_title('Test Accuracy Comparison', fontweight='bold')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    \n",
        "    axes[0, 1].bar(df['Model'], df['F1 (Macro)'], color='lightcoral')\n",
        "    axes[0, 1].set_title('F1 Score (Macro) Comparison', fontweight='bold')\n",
        "    axes[0, 1].set_ylabel('F1 Score')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    \n",
        "    axes[1, 0].bar(df['Model'], df['Training Time (min)'], color='lightgreen')\n",
        "    axes[1, 0].set_title('Training Time Comparison', fontweight='bold')\n",
        "    axes[1, 0].set_ylabel('Training Time (minutes)')\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    \n",
        "    metrics_cols = ['Accuracy', 'F1 (Macro)', 'Precision (Macro)', 'Recall (Macro)', 'AUC (Macro)']\n",
        "    df_normalized = df[metrics_cols].copy()\n",
        "    for col in metrics_cols:\n",
        "        df_normalized[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
        "    \n",
        "    for i, (_, row) in enumerate(df.iterrows()):\n",
        "        axes[1, 1].plot(metrics_cols, df_normalized.iloc[i][metrics_cols], \n",
        "                       marker='o', label=row['Model'], linewidth=2)\n",
        "    \n",
        "    axes[1, 1].set_title('Normalized Multi-Metric Comparison', fontweight='bold')\n",
        "    axes[1, 1].set_ylabel('Normalized Score')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "    axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "    best_accuracy = df.loc[df['Accuracy'].idxmax()]\n",
        "    best_f1 = df.loc[df['F1 (Macro)'].idxmax()]\n",
        "    fastest = df.loc[df['Training Time (min)'].idxmin()]\n",
        "    \n",
        "    print(f\" Top Performers:\")\n",
        "    print(f\"Best Accuracy: {best_accuracy['Model']} ({best_accuracy['Accuracy']:.4f})\")\n",
        "    print(f\"Best F1 Score: {best_f1['Model']} ({best_f1['F1 (Macro)']:.4f})\")\n",
        "    print(f\"Fastest Training: {fastest['Model']} ({fastest['Training Time (min)']:.1f} min)\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "def load_saved_results(results_file='results/phase1_results.json'):\n",
        "    \n",
        "    try:\n",
        "        with open(results_file, 'r') as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\" Results file {results_file} not found\")\n",
        "        return {}\n",
        "    except Exception as e:\n",
        "        print(f\" Error loading results: {e}\")\n",
        "        return {}\n",
        "\n",
        "def save_model_comparison_report(df, filename='results/model_comparison_report.csv'):\n",
        "    os.makedirs('results', exist_ok=True)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Model comparison report saved to {filename}\")\n",
        "\n",
        "print(\" Results analysis functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if 'phase1_results' in locals():\n",
        "    print(\" Analyzing Phase 1 training results...\")\n",
        "    comparison_df = compare_models(phase1_results)\n",
        "    if comparison_df is not None:\n",
        "        save_model_comparison_report(comparison_df)\n",
        "        \n",
        "else:\n",
        "    \n",
        "    print(\" Looking for saved results...\")\n",
        "    saved_results = load_saved_results()\n",
        "    \n",
        "    if saved_results:\n",
        "        print(\" Analyzing saved Phase 1 results...\")\n",
        "        comparison_df = compare_models(saved_results)\n",
        "        if comparison_df is not None:\n",
        "            save_model_comparison_report(comparison_df)\n",
        "    else:\n",
        "        print(\" No results available for analysis yet.\")\n",
        "        print(\" After training models, results will be automatically analyzed here.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
