{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import copy\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, roc_auc_score\n",
        ")\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Hyperparameters / Config\n",
        "IMG_SIZE = 320\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 25\n",
        "LR_HEAD = 1e-3\n",
        "LR_BACKBONE = 5e-5\n",
        "WEIGHT_DECAY = 1e-4\n",
        "LABEL_SMOOTH = 0.02\n",
        "HEAD_WARMUP_EPOCHS = 5\n",
        "USE_MIXUP = False\n",
        "MIXUP_ALPHA = 0.1\n",
        "USE_TTA = True\n",
        "EARLY_STOP_PATIENCE = 6\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset discovery from KaggleHub-style layout or explicit path\n",
        "EXPECTED_CLASSES = ['Non Demented', 'Very mild Dementia', 'Mild Dementia', 'Moderate Dementia']\n",
        "\n",
        "def resolve_dataset_path(root_candidates):\n",
        "    for root in root_candidates:\n",
        "        if not root:\n",
        "            continue\n",
        "        candidates = [os.path.join(root, 'Data'), root]\n",
        "        for cand in candidates:\n",
        "            if os.path.isdir(cand):\n",
        "                subdirs = [d for d in os.listdir(cand) if os.path.isdir(os.path.join(cand, d))]\n",
        "                found = [d for d in subdirs if d in EXPECTED_CLASSES]\n",
        "                if len(found) == len(EXPECTED_CLASSES):\n",
        "                    return cand, found\n",
        "                if len(found) > 0:\n",
        "                    return cand, found\n",
        "    return None, []\n",
        "\n",
        "\n",
        "DATASET_ROOT = os.environ.get('ALZ_DATASET_ROOT', None)\n",
        "\n",
        "\n",
        "common_roots = [\n",
        "    DATASET_ROOT,\n",
        "    os.path.expanduser(r\"~/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1\"),\n",
        "    os.path.expanduser(r\"~/kaggle/input/imagesoasis\"),\n",
        "    r\"C:\\\\Users\\\\sajib\\\\.cache\\\\kagglehub\\\\datasets\\\\ninadaithal\\\\imagesoasis\\\\versions\\\\1\",\n",
        "]\n",
        "\n",
        "dataset_path, class_names = resolve_dataset_path(common_roots)\n",
        "if not dataset_path:\n",
        "    raise FileNotFoundError(\"Could not resolve dataset path. Set ALZ_DATASET_ROOT to the dataset root or adjust common_roots.\")\n",
        "\n",
        "print(f\"Dataset path: {dataset_path}\")\n",
        "print(f\"Classes: {class_names}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class AlzheimerDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.class_to_idx = {\n",
        "            'Non Demented': 0,\n",
        "            'Very mild Dementia': 1,\n",
        "            'Mild Dementia': 2,\n",
        "            'Moderate Dementia': 3\n",
        "        }\n",
        "        if len(self.labels) > 0 and isinstance(self.labels[0], str):\n",
        "            self.labels = [self.class_to_idx[lbl] for lbl in self.labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.image_paths[idx]).convert('L')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = self.labels[idx]\n",
        "        return img, label\n",
        "\n",
        "\n",
        "def get_transforms(img_size=224, is_training=False):\n",
        "    if is_training:\n",
        "        return T.Compose([\n",
        "            T.Resize(int(img_size*1.14)),\n",
        "            T.CenterCrop(img_size),\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "            T.Grayscale(num_output_channels=3),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        return T.Compose([\n",
        "            T.Resize(int(img_size*1.14)),\n",
        "            T.CenterCrop(img_size),\n",
        "            T.Grayscale(num_output_channels=3),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def load_and_split(dataset_path, class_names, test_size=0.15, val_size=0.15, random_state=SEED):\n",
        "    image_paths, labels = [], []\n",
        "    for cls in class_names:\n",
        "        cls_dir = os.path.join(dataset_path, cls)\n",
        "        files = [f for f in os.listdir(cls_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        for f in files:\n",
        "            image_paths.append(os.path.join(cls_dir, f))\n",
        "            labels.append(cls)\n",
        "    print(f\"Total images: {len(image_paths)}\")\n",
        "\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        image_paths, labels, test_size=test_size, stratify=labels, random_state=random_state\n",
        "    )\n",
        "    val_size_adjusted = val_size / (1 - test_size)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=val_size_adjusted, stratify=y_temp, random_state=random_state\n",
        "    )\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
        "\n",
        "\n",
        "def make_loaders(dataset_path, class_names, batch_size=BATCH_SIZE, img_size=IMG_SIZE, num_workers=0):\n",
        "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = load_and_split(dataset_path, class_names)\n",
        "\n",
        "    train_tf = get_transforms(img_size, True)\n",
        "    eval_tf = get_transforms(img_size, False)\n",
        "\n",
        "    train_ds = AlzheimerDataset(X_train, y_train, train_tf)\n",
        "    val_ds = AlzheimerDataset(X_val, y_val, eval_tf)\n",
        "    test_ds = AlzheimerDataset(X_test, y_test, eval_tf)\n",
        "\n",
        "   \n",
        "    num_classes = 4\n",
        "    train_label_tensor = torch.tensor(train_ds.labels, dtype=torch.long)\n",
        "    binc = torch.bincount(train_label_tensor, minlength=num_classes).float()\n",
        "    binc[binc == 0] = 1.0\n",
        "    inv_sqrt = (1.0 / torch.sqrt(binc))\n",
        "    class_weights = (inv_sqrt / inv_sqrt.sum()) * num_classes  # moderate\n",
        "    print(f\"Class weights: {class_weights.numpy()}\")\n",
        "\n",
        "   \n",
        "    per_sample_w = (1.0 / torch.sqrt(binc))[train_label_tensor]\n",
        "    sampler = WeightedRandomSampler(weights=per_sample_w.double(), num_samples=len(per_sample_w), replacement=True\n",
        ")\n",
        "    loaders = {\n",
        "        'train': DataLoader(train_ds, batch_size=batch_size, sampler=sampler, shuffle=False, num_workers=num_workers, pin_memory=True, drop_last=True),\n",
        "        'val': DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True),\n",
        "        'test': DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True),\n",
        "        'class_weights': class_weights\n",
        "    }\n",
        "    return loaders, {'class_names': class_names}\n",
        "\n",
        "loaders, meta = make_loaders(dataset_path, class_names)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Models\n",
        "\n",
        "def create_model(name: str, num_classes: int = 4, pretrained: bool = True) -> nn.Module:\n",
        "    if name == 'vgg16':\n",
        "        m = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "        m.classifier[6] = nn.Linear(4096, num_classes)\n",
        "        return m\n",
        "    if name == 'vgg19':\n",
        "        m = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "        m.classifier[6] = nn.Linear(4096, num_classes)\n",
        "        return m\n",
        "    if name == 'resnet50':\n",
        "        m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None)\n",
        "        m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
        "        return m\n",
        "    if name == 'resnet101':\n",
        "        m = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V2 if pretrained else None)\n",
        "        m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
        "        return m\n",
        "    if name == 'resnet152':\n",
        "        m = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V2 if pretrained else None)\n",
        "        m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
        "        return m\n",
        "    if name == 'densenet121':\n",
        "        m = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "        m.classifier = nn.Linear(m.classifier.in_features, num_classes)\n",
        "        return m\n",
        "    if name == 'densenet201':\n",
        "        m = models.densenet201(weights=models.DenseNet201_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "        m.classifier = nn.Linear(m.classifier.in_features, num_classes)\n",
        "        return m\n",
        "    if name == 'mobilenetv3_large':\n",
        "        m = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V2 if pretrained else None)\n",
        "        m.classifier[3] = nn.Linear(m.classifier[3].in_features, num_classes)\n",
        "        return m\n",
        "    if name == 'shufflenet_v2_x1_0':\n",
        "        m = models.shufflenet_v2_x1_0(weights=models.ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "        m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
        "        return m\n",
        "    raise ValueError(f\"Unknown model: {name}\")\n",
        "\n",
        "ALL_MODELS = [\n",
        "    'vgg16', 'vgg19',\n",
        "    'resnet50', 'resnet101', 'resnet152',\n",
        "    'densenet121', 'densenet201',\n",
        "    'mobilenetv3_large', 'shufflenet_v2_x1_0'\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def one_hot_targets(labels, num_classes=4, smoothing=LABEL_SMOOTH):\n",
        "    with torch.no_grad():\n",
        "        true_dist = torch.zeros((labels.size(0), num_classes), device=labels.device)\n",
        "        true_dist.fill_(smoothing / (num_classes - 1))\n",
        "        true_dist.scatter_(1, labels.unsqueeze(1), 1.0 - smoothing)\n",
        "    return true_dist\n",
        "\n",
        "\n",
        "def mixup_data(x, y, alpha=MIXUP_ALPHA):\n",
        "    if alpha <= 0:\n",
        "        return x, y, 1.0\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, (y_a, y_b), lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, epoch):\n",
        "    \n",
        "    use_mixup = (epoch < max(HEAD_WARMUP_EPOCHS + 2, 4)) and USE_MIXUP\n",
        "    model.train()\n",
        "    scaler = GradScaler(enabled=torch.cuda.is_available())\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    pbar = tqdm(loader, desc=f\"Train {epoch+1}\")\n",
        "    for images, labels in pbar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with autocast('cuda', enabled=torch.cuda.is_available()):\n",
        "            if use_mixup:\n",
        "                images, (ya, yb), lam = mixup_data(images, labels, MIXUP_ALPHA)\n",
        "                outputs = model(images)\n",
        "                loss = mixup_criterion(criterion, outputs, ya, yb, lam)\n",
        "            else:\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = outputs.argmax(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += images.size(0)\n",
        "        pbar.set_postfix(loss=running_loss/total, acc=correct/total)\n",
        "    return running_loss/total, correct/total\n",
        "\n",
        "\n",
        "def eval_epoch(model, loader, criterion, epoch, phase=\"Val\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    pbar = tqdm(loader, desc=f\"{phase} {epoch+1}\")\n",
        "    all_probs, all_labels, all_preds = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in pbar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            with autocast('cuda', enabled=torch.cuda.is_available()):\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            preds = outputs.argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += images.size(0)\n",
        "            all_probs.append(probs.detach().cpu())\n",
        "            all_labels.append(labels.detach().cpu())\n",
        "            all_preds.append(preds.detach().cpu())\n",
        "            pbar.set_postfix(loss=running_loss/total, acc=correct/total)\n",
        "    all_probs = torch.cat(all_probs).numpy()\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    return running_loss/total, correct/total, all_probs, all_labels, all_preds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def train_model(name, loaders, num_epochs=NUM_EPOCHS, lr=LR_BACKBONE, weight_decay=WEIGHT_DECAY):\n",
        "    print(f\"\\n==== Training {name} ====\")\n",
        "    model = create_model(name, num_classes=4, pretrained=True).to(device)\n",
        "    class_weights = loaders['class_weights'].to(device)\n",
        "    base_criterion = nn.CrossEntropyLoss(weight=class_weights.to(device), label_smoothing=LABEL_SMOOTH)\n",
        "\n",
        "\n",
        "    head_params, backbone_params = [], []\n",
        "    for n, p in model.named_parameters():\n",
        "        if any(k in n for k in ['fc', 'classifier']):\n",
        "            head_params.append(p)\n",
        "        else:\n",
        "            backbone_params.append(p)\n",
        "    optimizer = optim.AdamW([\n",
        "        {'params': backbone_params, 'lr': lr},\n",
        "        {'params': head_params, 'lr': LR_HEAD}\n",
        "    ], weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "    best_acc = 0.0\n",
        "    best_state = None\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        if epoch < HEAD_WARMUP_EPOCHS:\n",
        "            for p in backbone_params:\n",
        "                p.requires_grad = False\n",
        "        else:\n",
        "            for p in backbone_params:\n",
        "                p.requires_grad = True\n",
        "\n",
        "        tr_loss, tr_acc = train_epoch(model, loaders['train'], base_criterion, optimizer, epoch)\n",
        "        va_loss, va_acc, va_probs, va_labels, va_preds = eval_epoch(model, loaders['val'], base_criterion, epoch, phase='Val')\n",
        "        history['train_loss'].append(tr_loss)\n",
        "        history['train_acc'].append(tr_acc)\n",
        "        history['val_loss'].append(va_loss)\n",
        "        history['val_acc'].append(va_acc)\n",
        "        scheduler.step(va_loss)\n",
        "        # extra metrics\n",
        "        va_precision = precision_score(va_labels, va_preds, average='macro', zero_division=0)\n",
        "        va_recall = recall_score(va_labels, va_preds, average='macro', zero_division=0)\n",
        "        va_f1 = f1_score(va_labels, va_preds, average='macro', zero_division=0)\n",
        "        print(f\"Epoch {epoch+1}: train_acc={tr_acc:.4f} val_acc={va_acc:.4f} val_f1={va_f1:.4f} val_prec={va_precision:.4f} val_rec={va_recall:.4f}\")\n",
        "\n",
        "        # per-epoch CSV logging\n",
        "        os.makedirs('results', exist_ok=True)\n",
        "        backbone_lr = optimizer.param_groups[0]['lr']\n",
        "        head_lr = optimizer.param_groups[1]['lr']\n",
        "        log_row = {\n",
        "            'model_name': name,\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': tr_loss,\n",
        "            'train_acc': tr_acc,\n",
        "            'val_loss': va_loss,\n",
        "            'val_acc': va_acc,\n",
        "            'val_f1_macro': va_f1,\n",
        "            'val_precision_macro': va_precision,\n",
        "            'val_recall_macro': va_recall,\n",
        "            'lr_backbone': lr,\n",
        "            'lr_head': LR_HEAD,\n",
        "            'img_size': IMG_SIZE,\n",
        "            'batch_size': BATCH_SIZE,\n",
        "            'label_smooth': LABEL_SMOOTH,\n",
        "            'warmup_epochs': HEAD_WARMUP_EPOCHS,\n",
        "            'mixup': USE_MIXUP\n",
        "        }\n",
        "        log_path = os.path.join('results', 'epoch_log.csv')\n",
        "        pd.DataFrame([log_row]).to_csv(log_path, mode='a', header=not os.path.exists(log_path), index=False)\n",
        "\n",
        "        if va_acc > best_acc:\n",
        "            best_acc = va_acc\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= EARLY_STOP_PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch+1} (no val acc improvement for {EARLY_STOP_PATIENCE} epochs)\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    te_loss, te_acc, te_probs, te_labels, te_preds = eval_epoch(model, loaders['test'], base_criterion, -1, phase='Test')\n",
        "\n",
        "    if USE_TTA:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            tta_probs = []\n",
        "            for images, _ in tqdm(loaders['test'], desc='TTA'):\n",
        "                images = images.to(device)\n",
        "                p1 = F.softmax(model(images), dim=1)\n",
        "                p2 = F.softmax(model(torch.flip(images, dims=[3])), dim=1)\n",
        "                tta_probs.append(((p1 + p2) / 2).cpu())\n",
        "            te_probs = torch.cat(tta_probs).numpy()\n",
        "\n",
        "    try:\n",
        "        y_true_bin = F.one_hot(torch.tensor(te_labels), num_classes=4).numpy()\n",
        "        auc_macro = roc_auc_score(y_true_bin, te_probs, average='macro', multi_class='ovr')\n",
        "    except Exception:\n",
        "        auc_macro = 0.0\n",
        "\n",
        "    # detailed test metrics\n",
        "    te_precision = precision_score(te_labels, te_preds, average='macro', zero_division=0)\n",
        "    te_recall = recall_score(te_labels, te_preds, average='macro', zero_division=0)\n",
        "    te_f1 = f1_score(te_labels, te_preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"Test: acc={te_acc:.4f} f1={te_f1:.4f} prec={te_precision:.4f} rec={te_recall:.4f} auc_macro={auc_macro:.4f}\")\n",
        "    print(f\"Model: {name} | ImgSize: {IMG_SIZE} | Batch: {BATCH_SIZE} | Epochs: {NUM_EPOCHS} | LR(head/backbone): {LR_HEAD}/{LR_BACKBONE} | Warmup: {HEAD_WARMUP_EPOCHS}\")\n",
        "\n",
        "    return {\n",
        "        'model_name': name,\n",
        "        'best_val_acc': best_acc,\n",
        "        'test_acc': te_acc,\n",
        "        'test_f1_macro': te_f1,\n",
        "        'test_precision_macro': te_precision,\n",
        "        'test_recall_macro': te_recall,\n",
        "        'test_auc_macro': auc_macro,\n",
        "        'config': {\n",
        "            'img_size': IMG_SIZE,\n",
        "            'batch_size': BATCH_SIZE,\n",
        "            'num_epochs': NUM_EPOCHS,\n",
        "            'lr_head': LR_HEAD,\n",
        "            'lr_backbone': LR_BACKBONE,\n",
        "            'weight_decay': WEIGHT_DECAY,\n",
        "            'label_smooth': LABEL_SMOOTH,\n",
        "            'warmup_epochs': HEAD_WARMUP_EPOCHS,\n",
        "            'mixup': USE_MIXUP\n",
        "        },\n",
        "        'history': history,\n",
        "        'state_dict': best_state\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "results = []\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "\n",
        "priority_models = [\n",
        "    'vgg16', 'vgg19','resnet50', 'resnet101', 'resnet152',\n",
        "    'densenet121', 'densenet201',\n",
        "    'mobilenetv3_large', 'shufflenet_v2_x1_0'\n",
        "]\n",
        "\n",
        "for name in priority_models:\n",
        "    try:\n",
        "        print(f\"Training {name}\")\n",
        "        res = train_model(name, loaders)\n",
        "        results.append(res)\n",
        "        torch.save({\n",
        "            'model_name': name,\n",
        "            'state_dict': res['state_dict'],\n",
        "            'history': res['history'],\n",
        "            'meta': meta,\n",
        "            'results': {k: res[k] for k in ['best_val_acc', 'test_acc', 'test_auc_macro']}\n",
        "        }, os.path.join('models', f'{name}_finetuned.pth'))\n",
        "    except Exception as e:\n",
        "        print(f\"Error training {name}: {e}\")\n",
        "\n",
        "\n",
        "if results:\n",
        "    df = pd.DataFrame([\n",
        "        {\n",
        "            'model': r['model_name'],\n",
        "            'val_acc': r['best_val_acc'],\n",
        "            'test_acc': r['test_acc'],\n",
        "            'test_f1_macro': r.get('test_f1_macro', None),\n",
        "            'test_precision_macro': r.get('test_precision_macro', None),\n",
        "            'test_recall_macro': r.get('test_recall_macro', None),\n",
        "            'test_auc_macro': r['test_auc_macro'],\n",
        "            'img_size': r['config']['img_size'],\n",
        "            'batch_size': r['config']['batch_size'],\n",
        "            'epochs': r['config']['num_epochs'],\n",
        "            'lr_head': r['config']['lr_head'],\n",
        "            'lr_backbone': r['config']['lr_backbone'],\n",
        "            'weight_decay': r['config']['weight_decay'],\n",
        "            'label_smooth': r['config']['label_smooth'],\n",
        "            'warmup_epochs': r['config']['warmup_epochs'],\n",
        "            'mixup': r['config']['mixup']\n",
        "        }\n",
        "        for r in results\n",
        "    ])\n",
        "    print(df.sort_values('test_acc', ascending=False))\n",
        "    df.to_csv('models/summary.csv', index=False)\n",
        "else:\n",
        "    print(\"No models trained.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
